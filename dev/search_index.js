var documenterSearchIndex = {"docs":
[{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"In this package are developped different loss functions:","category":"page"},{"location":"Loss/#Lagrangian-Sub-Problem-Loss","page":"Loss Functions","title":"Lagrangian Sub-Problem Loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_LR is the one presented in the paper [1] and consist on the bound provided by the Lagrangian Sub-Problem (with a proper sign that allows to write the Lagrangian Dual as minimization problem).","category":"page"},{"location":"Loss/#Lagrangian-Sub-Problem-Loss-on-GPU","page":"Loss Functions","title":"Lagrangian Sub-Problem Loss on GPU","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_LR_gpu: as LRloss but the sub-problem is solved in GPU.","category":"page"},{"location":"Loss/#Warning:-for-the-moment-this-loss-function-works-only-for-MCND-instances-with-GPU-memorization.","page":"Loss Functions","title":"Warning: for the moment this loss function works only for MCND instances with GPU memorization.","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"Anyway it is not faster than the one that use CPU.","category":"page"},{"location":"Loss/#GAP-Loss","page":"Loss Functions","title":"GAP Loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_GAP this loss function simply consists on the GAP of percentage  the value v provided by the Lagrangian sub-Problem and the optimal value of the Lagrangian dual and can be computed as:","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"fracv-v^*v^**100","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"when the Lagrangian Dual is a minimization problem","category":"page"},{"location":"Loss/#GAP-Closure-Loss","page":"Loss Functions","title":"GAP Closure Loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_GAP_closure this loss function is similar to GAPloss as still consider the value v provided by the Lagrangian sub-Problem and the optimal value of the Lagrangian dual. But it tries to further compare these solutions with the continuous relaxation bound CR. It is computed as:","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"fracvv^*-CR * 100","category":"page"},{"location":"Loss/#Hinge-Loss","page":"Loss Functions","title":"Hinge Loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"For an instance iota in I with gold solution (x^* y^*) (more precisely (x^*(iota) y^*(iota))) of L(pi^*), the Hinge loss is","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"H(wiota) =  L(pi(w) x^* y^*) - min_xy Big(L(pi(w) x y) - alpha Delta_y^*(y)Big)","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"where w are the parameters of the model, pi(w) is the prediction of the model given w, (x^*y^*)  is the gold solution of the instance,  Delta_y^*(y) is the hamming loss between y and y^* and alpha is a non negative scalar.","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"The loss_hinge is ","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"frac1Isum_iota in I frac1A(iota)H(wiota)","category":"page"},{"location":"Loss/#Mean-Squared-Error","page":"Loss Functions","title":"Mean Squared Error","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_mse is only the MSE between the mean squared error between the predicted and the optimal Lagrangian Multipliers.","category":"page"},{"location":"Loss/#Multi-Prediction-LR-loss","page":"Loss Functions","title":"Multi Prediction LR loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_multi_LR_factory is a specialized version of loss_LR that is able to handle with Multiple Lagrangian Multipliers Predictions.","category":"page"},{"location":"Loss/#More-sophisticated-variants-of-this-loss-function-will-be-provided-in-the-future.","page":"Loss Functions","title":"More sophisticated variants of this loss function will be provided in the future.","text":"","category":"section"},{"location":"Loss/#References","page":"Loss Functions","title":"References","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"[1]: F. Demelas, J. Le Roux, M. Lacroix, A. Parmentier \"Predicting Lagrangian Multipliers for Mixed Integer Linear Programs\", ICML 2024.","category":"page"},{"location":"api/#API-for-learning_pi.jl","page":"API reference","title":"API for learning_pi.jl","text":"","category":"section"},{"location":"api/#Index","page":"API reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"","category":"page"},{"location":"api/#Public","page":"API reference","title":"Public","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [LearningPi]\nPrivate = false","category":"page"},{"location":"api/#LearningPi.createCorpus","page":"API reference","title":"LearningPi.createCorpus","text":"createCorpus(featType::learningType, folder::String, maxInstance::Vector{Int64}=[-1, -1, -1],seed::Int64=0;factory::abstractInstanceFactory=cpuMCNDinstanceFactory(),pTrain::Float32=0.8,pVal::Float32=0.1)\n\nArguments:\n\nfeatType : the type of the features instance.\nfolder : the path to the directory that contains the json files that defines the instances (and the associated features and labels).\nmaxInstance : a vector with three components.   that say how many instances take for the training/validation/test set.\nseed : a random seed used to select which instaces consider in the training/validation/test sets.\nfactory: type of instance.\npTrain : The percentage of training instances in the provided folder.\npVal : The percentage of validation instances in the provided folder.\n\nCreate a Corpus, that is a structure with three DataSet field for training,validation and test dataset. Note: The percentage for the test set will be 1-pTrain-pVal. It is important to select the percentage of training and validation in such a way that pTrain+pVal<1 and both will be non-negative.\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningPi.createDataSet","page":"API reference","title":"LearningPi.createDataSet","text":"createDataSet(lt::learningType, directory, maxInstance::Int64=-1,factory::abstractInstanceFactory=cpuMCNDinstanceFactory())\n\nArguments:\n\nlt: the learning type.\ndirectory: a list of paths to the instances.\nmaxInstance: the maximum number of instances that we wat consider in the provided directory. By default is equal to -1, that means consider all the instances in the directory.\nfactory: type of instance, the possibilities in this moment are cpuMCNDinstanceFactory() (that is Multi-Commodity Network-Design instances) and cpuCWLinstanceFactory() (for the Bin Packing instances).\n\nCreate the dataset for the provided (general) learning type. return a dataSet structure of a proper type.\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningPi.createDataSet-2","page":"API reference","title":"LearningPi.createDataSet","text":"function createDataSet(lt::learningGNN, directory::String, maxInstance::Int64=-1, factory::abstractInstanceFactory)\n\n#Arguments:\n\t- `lt`:learning type, it should be a sub-type learningGNN \n\t- `directory`: the path to the directory containing instances\n\t- `maxInstance`: maximum instance number \n\t- `factory`: instance factory, generic sub-type of abstractInstanceFactory\n\nCreate and returns a dataset for the provided learning type `lt`, considering `maxInst` instances of the factory `factory`, contained in `directory`\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningPi.createKfold","page":"API reference","title":"LearningPi.createKfold","text":"createKfold(featType::learningType, folder::String, maxInstance::Vector{Int64}=[-1, -1, -1],seed::Int64=0;factory::abstractInstanceFactory=cpuMCNDinstanceFactory(),k::Int64=1)\n\nArguments:\n\nfeatType : the type of the features instance.\nfolder : the path to the directory that contains the json files that defines the instances (and the associated features and labels).\nmaxInstance : a vector with three components.   that say how many instances take for the training/validation/test set.\nseed : a random seed used to select which instaces consider in the training/validation/test sets.\nfactory: type of instance.\nk: the fold that we want select as test set. Note: 1 <= k <= 10.\n\nCreate a Corpus, that is a struct with three DataSet field for training/validation/test set\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningPi.createLabels-Tuple{Any, Any, Any, Any, Any, abstractInstanceMCND}","page":"API reference","title":"LearningPi.createLabels","text":"createLabels(π, x, y, LRarcs, objLR, ins::abstractInstanceMCND)\n\nArguments:\n\nπ: a (optimal) Lagrangian multipliers vector\nx: the flow variables in the Lagrangian sub-problem, obtained afer the resolution of the sub-problem with multipliers π\ny: the design variables in the Lagrangian sub-problem, obtained afer the resolution of the sub-problem with multipliers π\nLRarcs: a vector containign the bounds for each edge of the Lagrangian sub-problem considering π as Lagrangian multipleirs vector\nobjLR: the bound of the Lagrangian sub-problem considering π as Lagrangian multipleirs vector\nins: the instance structure (standard instance formulation, without regularization).\n\nFor instance (not normalized), nothing to do, just return a proper label structure.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.createLabels-Tuple{Any, Any, Any, Any, instanceCWL}","page":"API reference","title":"LearningPi.createLabels","text":"function createLabels(π, x, y, objLR, ins::instanceCWL)\n\n#Arguments:\n\n-π: optimal lagrangian multipliers vector. -x: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the optimal Lagrangian multipliers). -y: primal solution of the Knapsack Lagrangian Relaxation associated to the variables say if we use or not a pack (using the optimal Lagrangian multipliers). -objLR: optimal value of the Lagrangian Dual\n\nins: instance object, it should be of type sub-type of instanceCWL \n\nGiven all the fields construct a label structure for the Bin Packing Problem.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.createLabels-Tuple{Any, Any, Any, Instances.instanceGA}","page":"API reference","title":"LearningPi.createLabels","text":"function createLabels(π, x, objLR, ins::instanceGA)\n\n#Arguments: \t-π: optimal lagrangian multipliers vector. \t-x: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the optimal Lagrangian multipliers). \t-objLR: optimal value of the Lagrangian Dual\n\nGiven all the fields construct a label structure for the Bin Packing Problem.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.create_example-Tuple{LearningPi.learningGNN, String, abstractInstanceFactory, LearningPi.abstract_features_matrix}","page":"API reference","title":"LearningPi.create_example","text":"create_example(lt::learningGNN, fileName::String, factory::abstractInstanceFactory)\n\n#Arguments:\n\nlt: learning Type, this function works for all the learning types that use a graph representation of the instance\nfileName: path to the json that contains all the information to construct the learning sample starting from an instance, its features and the labels.\nfactory: instance factory, it works for all the factory\n\nreturns an gnnExample_instance with all the information useful for the training.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.create_example-Tuple{LearningPi.learningMLP, String, abstractInstanceFactory, Any}","page":"API reference","title":"LearningPi.create_example","text":"create_example(lt::learningArc, fileName::String,factory::abstractInstanceFactory)\n\nArguments:\n\nlt: learning type, it should be learningMLP.\nfileName: the name of the file json that contains the informations about the instance, its features and its labels.\nfactory: type of instance (it handle both with the normalized and un-normalized instances).\n\nCreate a structure containing the instance, the extracted features and the associated labels.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.create_loss","page":"API reference","title":"LearningPi.create_loss","text":"createloss(::HingeLoss)\n\nArguments:\n\n-:: loss parameters, it should be a structure of type HingeLoss.   \n\nreturn the loss correspondent to loss paramameters of type HingeLoss\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningPi.create_model","page":"API reference","title":"LearningPi.create_model","text":"create_model(lType::learningMLP, in, h, out=1, a=relu)\n\nArguments:\n\nlType: general learningType.\nin: size of the input layer.\nh: a vector with the same length as the desired number of hidden layers and each component say how many nodes we want in the correspondent hidden layer.\nout: size of the output layer, by default is equal to one.\na: the activation function for the hidden layers, by default is relu.\n\nThis function creates a model for the provided learning type (in order to use this variant it should be learningArc). The model is a multi layer perceptron with in nodes in the first layer, length(h) hidden layers (the i-th layer has h[i] nodes) and out nodes in the output layer (by default 1). By default each hidden layer use a relu activation function (the input and output layers have no activation function).   \n\n\n\n\n\n","category":"function"},{"location":"api/#LearningPi.dataLoader-Tuple{String, Instances.cpuGAinstanceFactory}","page":"API reference","title":"LearningPi.dataLoader","text":"dataLoader(fileName::String, factory::cpuGAinstanceFactory)\n\nArguments\n\n- `fileName` : a path to a json file that contains the data for the instance, features and labels\n- `factory` : an instance factory for the Bin Packing problem\n\nIt reads the instance, the features and the labels from the json and returns three structures that contains all the informations.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.dataLoader-Tuple{String, MCNDinstanceFactory}","page":"API reference","title":"LearningPi.dataLoader","text":"dataLoader(fileName::String, factory::MCNDnstanceFactory)\n\nArguments\n\n- `fileName` : a path to a json file that contains the data for the instance, features and labels\n- `factory` : an instance object (for the same instance as the features file)\n\nIt reads the instance, the features and the labels from the json and returns three structures that contains all the informations.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.dataLoader-Tuple{String, cpuCWLinstanceFactory}","page":"API reference","title":"LearningPi.dataLoader","text":"dataLoader(fileName::String, factory::cpuCWLinstanceFactory)\n\nArguments\n\n- `fileName` : a path to a json file that contains the data for the instance, features and labels\n- `factory` : an instance factory for the Bin Packing problem\n\nIt reads the instance, the features and the labels from the json located in fileName and returns three structures that contains all the informations.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.featuresExtraction-Tuple{LearningPi.learningMLP, Any, abstractInstance}","page":"API reference","title":"LearningPi.featuresExtraction","text":"featuresExtraction(featType::learningMLP, featObj, ins)\n\nVectorization function for the features when we consider a learningNodeDemand encoding.\n\nArguments:\n\nfeatType features type\nfeatures a features matrix\nnbFeatures the number of features\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.featuresExtraction-Tuple{LearningPi.learningType, Any, Instances.instanceGA, LearningPi.abstract_features_matrix}","page":"API reference","title":"LearningPi.featuresExtraction","text":"function featuresExtraction(lt::learningType, featObj, ins::instanceGA, fmt::abstractfeaturesmatrix)\n\n#Arguments:\n\nlt: learnign type, it should be a sub-type of learningType\nfeatObj: features object containing all the characteristics \nins: instance structure, it should instanceGA\nfmt: features matrix type.\n\nReturns the bipartite graph representation with the associated nodes-features matrix.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.featuresExtraction-Tuple{LearningPi.learningType, Any, abstractInstanceMCND, LearningPi.abstract_features_matrix}","page":"API reference","title":"LearningPi.featuresExtraction","text":"function featuresExtraction(lt::learningGNN, featObj, ins::abstractInstanceMCND)\n\nArguments:\n\nlt: learnign type, it should be a sub-type of learningGNN\nfeatObj: features object containing all the characteristics \nins: instance structure, it should be a sub-type of abstractInstanceMCND\n\nReturns the bipartite graph representation with the associated nodes-features matrix.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.featuresExtraction-Tuple{LearningPi.learningType, Any, instanceCWL, LearningPi.abstract_features_matrix}","page":"API reference","title":"LearningPi.featuresExtraction","text":"function featuresExtraction(lt::learningType, featObj, ins::instanceCWL,fmt::abstractfeaturesmatrix)\n\n#Arguments:\n\nlt: learnign type, it should be a sub-type of learningType\nfeatObj: features object containing all the characteristics \nins: instance structure, it should instanceCWL\nfmt: features matrix type.\n\nReturns the bipartite graph representation with the associated nodes-features matrix.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.prediction-Tuple{Flux.Chain, Any, Any, LearningPi.learningMLP, LearningPi.abstract_deviation}","page":"API reference","title":"LearningPi.prediction","text":"prediction(nn, f,ins,lt::learningMLP)\n\nArguments:\n\n-nn: neural network model. -f: features matrix. -ins: structure containing the instance informations. -lt: learning type (general).\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.train-Tuple{Int64, LearningPi.Corpus, Any, Flux.Optimise.Optimiser, LearningPi.abstract_loss}","page":"API reference","title":"LearningPi.train","text":"function train(maxEp::Int64, dS::Corpus, nn, opt::Optimiser, loss::abstractLoss; printEpoch::Int64=10, endString::String, lt::learningType, dt::abstract_deviation,seed::Int64,bs::Int64=1)\n\nArguments:\n\nmaxEp: the maximum number of epochs for the learning algorithm.\ndS: the Corpus structure that contains the training, validation and test sets.\nnn: the neural network model.\nopt: the optimizer used for the training.\nloss: a structure that contains the parameters α and β of the loss.\nprintEpoch: the number of epochs in which print the metrics of the training.\nendString: the string used to memorize the output files as best models and tensorboard logs.\ndt: deviation type, it could deviate from zero or the duals of the continuous relaxation.\nlt: learning type\nseed: random seed for the random generators\nbs batch size\n\nThis function performs the learning with the provided inputs and save the best models in a bson file.\n\n\n\n\n\n","category":"method"},{"location":"api/#Private","page":"API reference","title":"Private","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [LearningPi]\nPublic= false","category":"page"},{"location":"api/#LearningPi.Corpus","page":"API reference","title":"LearningPi.Corpus","text":"Corpus structure contains the three datasets: training, validation and test set\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.Graphormer-Tuple{Any}","page":"API reference","title":"LearningPi.Graphormer","text":"function (m::Graphormer)(x)\n\nArguments:\n\nx: input of the NN model of type Graphormer    \n\nForward computation of a Graphormer m, the output is the concatenation of all the multipliers predicted by the model   \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.LayerNorm","page":"API reference","title":"LearningPi.LayerNorm","text":"struct LayerNorm\n\n#Fields:\n\neps: regularization parameter\nd: size of the input of the normalization layer \n\nDescribe the Layer Normalization for the provided parameters\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.LayerNorm-Tuple{Any}","page":"API reference","title":"LearningPi.LayerNorm","text":"function (m::LayerNorm)(x)\n\nPerform a Layer Normalization using x as input .\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.RMSNorm","page":"API reference","title":"LearningPi.RMSNorm","text":"struct RMSNorm\n\n#Fields:\n\neps: additive regularization parameter\nsqrtd: multiplicative regularization parameter\n\nDescribe the RMS Normalization for the provided parameters\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.abstract_dataset","page":"API reference","title":"LearningPi.abstract_dataset","text":"Abstract type for the dataset\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.abstract_deviation","page":"API reference","title":"LearningPi.abstract_deviation","text":"abstract type abstract_deviation end\n\nAbstract type to handle the deviation vector, i.e. the starting point from which our model produce an additive activation. \n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.abstract_example","page":"API reference","title":"LearningPi.abstract_example","text":"Abstract type for an element of a dataset\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.abstract_features_matrix","page":"API reference","title":"LearningPi.abstract_features_matrix","text":"abstract type abstract_features_matrix\n\nAbstract type for the construction of the nodes-features matrix associated to the bipartite graph representation of the instance.\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.cr_deviation","page":"API reference","title":"LearningPi.cr_deviation","text":"struct cr_deviation end\n\nType to use as deviation vector (i.e. the starting point from which our model produce an additive activation) the dual variables associated to the relaxed constraints in the optimal solution of the Continuous Relaxation. \n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.cr_features_matrix","page":"API reference","title":"LearningPi.cr_features_matrix","text":"struct cr_features_matrix\n\nwith this choice the features matrix considers the informations related to the continuous relaxation of the instance.\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.featuresCWL","page":"API reference","title":"LearningPi.featuresCWL","text":"Features structure for the Bin Packing instance. #Fields: -xCR: primal solution of the Linear Relaxation associated to the variables that associate one items to a pack. -yCR: primal solution of the Linear Relaxation associated to the variables say if we use or not a pack. -λ: dual solution of the Linear Relaxation associated to the packing constraints. -μ: dual solution of the Linear Relaxation associated to the packing constraints. -objCR: objective value of the Linear Relaxation. -xLR: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the dual variables λ of the linear relaxation). -yLR: primal solution of the Knapsack Lagrangian Relaxation associated to the variables say if we use or not a pack (using the dual variables λ of the linear relaxation). -objLR: objective value of the Knapsack Lagrangian Relaxation (using the dual variables λ of the linear relaxation). \n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.featuresGA","page":"API reference","title":"LearningPi.featuresGA","text":"Features structure for the Bin Packing instance. #Fields: -xCR: primal solution of the Linear Relaxation associated to the variables that associate one items to a pack. -λ: dual solution of the Linear Relaxation associated to the packing constraints. -μ: dual solution of the Linear Relaxation associated to the packing constraints. -objCR: objective value of the Linear Relaxation. -xLR: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the dual variables λ of the linear relaxation). -objLR: objective value of the Knapsack Lagrangian Relaxation (using the dual variables λ of the linear relaxation). \n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.featuresMCND","page":"API reference","title":"LearningPi.featuresMCND","text":"Struct containing the information of the features for an instance.\n\nFields:\n\nxCR: the value of the flow variables for the optimal solution of the linear relaxation.\nyCR: the value of the decision variables for the optimal solution of the linear relaxation.\nλ: the value of the dual variables associated to the flow constraints, for the optimal solution of the linear relaxation.\nμ: the value of the dual variables associated to the capacity constraints for the optimal solution of the linear relaxation.\nobjCR: the objective value of the linear relaxation.\nxLR: the value of the flow variables for the optimal solution of the sub-problem for the knapsack relaxation, considering as lagrangian multiers the vector λ.\nyLR: the value of the design variables for the optimal solution of the sub-problem for the knapsack relaxation, considering as lagrangian multiers the vector λ.\nLRarcs: the objective values, for each edge, of the optimal solution of the sub-problem for the knapsack relaxation, considering as lagrangian multiers the vector λ.\nobjLR: the objective value of the sub-problem for the knapsack relaxation, considering as lagrangian multiers the vector λ.\norigins: a matrix of size K×V the cost of the shortest path from the origin to the current node with costs in an edge e:  ins.r[k,e]+ins.f[e]/ins.c[e].\ndestinations: a matrix of size K×V the cost of the shortest path from the current node to the destination with costs in an edge e:  ins.r[k,e]+ins.f[e]/ins.c[e].\ndistance: a matrix of size V×V with the distance in terms of number of edges for the shortest path from each two nodes.\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.labels","page":"API reference","title":"LearningPi.labels","text":"Struct containing the information relative to the labels\n\nFields\n\nπ::Matrix{Float32}: matrix containing the gold Lagrangian multipliers. π[k, i] gives the values of the Lagrangian multiplier associated with demand k and node i.\nx::Matrix{Int16}: solution x of the Lagrangian problem. x[k, a] gives the value of the solution x_a^k of the Lagrangian problem L(π) for demand k and arc a.\ny::Vector{Int16}: solution y of the Lagrangian problem. y[a] gives the value of the solution y_a of the Lagrangian problem L(π) for arc a.\nLRarcs::Vector{Float32}: Values of the Lagrangian problem for the arcs. LRarcs[a] gives the value of subproblem L_a associated with arc a.\nobjLR::Float32: Value of the Lagrangian dual problem\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.labelsCWL","page":"API reference","title":"LearningPi.labelsCWL","text":"Label structure for the Bin Packing Problem. \t#Fields: \t-π: optimal lagrangian multipliers vector. \t-xLR: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the optimal Lagrangian multipliers). \t-yLR: primal solution of the Knapsack Lagrangian Relaxation associated to the variables say if we use or not a pack (using the optimal Lagrangian multipliers). \t-objLR: optimal value of the Lagrangian Dual\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.labelsGA","page":"API reference","title":"LearningPi.labelsGA","text":"Label structure for the Generalized Assignment Problem. \t#Fields: \t-π: optimal lagrangian multipliers vector. \t-xLR: primal solution of the Lagrangian Subproblem with optimal Lagrangian multipliers. \t-objLR: optimal value of the Lagrangian Dual.\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.learningType","page":"API reference","title":"LearningPi.learningType","text":"abstract learning type to type the functions that should work with all the type of models and features encoding\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.loss_hinge","page":"API reference","title":"LearningPi.loss_hinge","text":"Structure of parameters for loss obtained as the inverse of the sub-problem obj value.\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.loss_hinge-Tuple{Any}","page":"API reference","title":"LearningPi.loss_hinge","text":"lossHinge(π; example, _::HingeLoss)\n\nLoss function obtained taking the inverse of the sub-problem value for the predicted lagrangians.\n\nArguments:\n\nπ: lagrangian multipliers vector candidate.\nexample: dataset sample object.\n\n-_: loss parameters, it should be a structure of type HingeLoss.   \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.loss_mse-Tuple{Any}","page":"API reference","title":"LearningPi.loss_mse","text":"loss_mse(π; example)\n\nArguments:\n\nπ: lagrangian multipliers vector candidate.\nexample: dataset sample object.\n\n-_: loss parameters, it should be a structure of type MSELoss.   \n\nreturns the loss function value obtained taking the MSE beteern the predicted Lagrangian multipliers π and the optimal ones in example.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.lr_features_matrix","page":"API reference","title":"LearningPi.lr_features_matrix","text":"struct lr_features_matrix\n\nwith this choice the features matrix considers the informations related to the continuous relaxation of the instance.\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.without_cr_features_matrix","page":"API reference","title":"LearningPi.without_cr_features_matrix","text":"struct withou_cr_features_matrix\n\nwith this choice the features matrix does not considers the informations related to the continuous relaxation of the instance.\n\n\n\n\n\n","category":"type"},{"location":"api/#LearningPi.zero_deviation","page":"API reference","title":"LearningPi.zero_deviation","text":"struct zero_deviation end\n\nType to use as deviation vector (i.e. the starting point from which our model produce an additive activation) the all zeros vector. \n\n\n\n\n\n","category":"type"},{"location":"api/#ChainRulesCore.rrule-Tuple{LearningPi.loss_LR, AbstractArray}","page":"API reference","title":"ChainRulesCore.rrule","text":"Compute the value of the Learning by Experience loss (usining the inverse of value of the sub-problem) and its pullback function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ChainRulesCore.rrule-Tuple{LearningPi.loss_LR_gpu, AbstractArray}","page":"API reference","title":"ChainRulesCore.rrule","text":"Compute the value of the Learning by Experience loss (usining the inverse of value of the sub-problem) and its pullback function.\n\n\n\n\n\n","category":"method"},{"location":"api/#ChainRulesCore.rrule-Tuple{LearningPi.loss_hinge, AbstractArray}","page":"API reference","title":"ChainRulesCore.rrule","text":"Compute the value of the Learning by Experience loss (usining the inverse of value of the sub-problem) and its pullback function.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.compareWithBests-Tuple{Dict, Dict, Any, Any}","page":"API reference","title":"LearningPi.compareWithBests","text":"function compareWithBests(currentMetrics::Dict,bestMetrics::Dict,nn,endString::String)\n\nArguments:\n\ncurrentMetrics : a dictionary of Float \nbestMetrics : a dictionary of Float \nnn : a neural network\nendString : a string used to memorize the best models\n\nThis function compare all the values in bestMetrics with the ones in currentMetrics (that corresponds to the same key). If some value in currentMetrics is better, then we update the correspondent value in bestMetrics and we save the model in a bson file.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.createEmptyDataset-Tuple{LearningPi.learningGNN}","page":"API reference","title":"LearningPi.createEmptyDataset","text":"createEmptyDataset(lt::learningGNN)\n\nArguments:\n\nlt: learning Multi Layer Perceptron type.\n\nCreate an empty dataset for the Graph Neural network learning type.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.createEmptyDataset-Tuple{LearningPi.learningMLP}","page":"API reference","title":"LearningPi.createEmptyDataset","text":"createEmptyDataset(lt::learningMLP)\n\nArguments:\n\nlt: learning Multi Layer Perceptron type.\n\nCreate an empty dataset for the  Multi Layer Perceptron learning type.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.create_features-Tuple{Instances.instanceGA}","page":"API reference","title":"LearningPi.create_features","text":"function create_features(ins::instanceGA)\n\n#Arguments:\n\t- `ins`: instance object, it should be of type instanceGA \n\nread the features and returns a features structure.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.create_features-Tuple{cpuInstanceMCND}","page":"API reference","title":"LearningPi.create_features","text":"function create_features(ins::cpuInstanceMCND)\n\n#Arguments:\n\nins: instance structure, should be of type cpuInstanceMCND\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.create_features-Tuple{instanceCWL}","page":"API reference","title":"LearningPi.create_features","text":"function create_features(ins::instanceCWL)\n\n#Arguments:\n\t- `ins`: instance object, it should be of type instanceCWL \n\nsolve the Continuous Relaxation and the Lagrangian Sub-Problem considering as Lagrangian Multipliers\nthe dual variables associated to the relaxed constraints and then returns a features structure.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.create_rms-Tuple{Any}","page":"API reference","title":"LearningPi.create_rms","text":"function create_rms(d)\n\n#Arguments:\n\nd: size of input in the input layers\n\nreturn a RMSNorm function.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.deviationFrom-Tuple{Any, LearningPi.cr_deviation}","page":"API reference","title":"LearningPi.deviationFrom","text":"function deviationFrom(x,::crdeviation)\n\n#Arguments:\n\nx: the bipartite-graph representation of the instance\n\nFor the cr_deviation it returns the dual variables associated to the dualized constraints in the optimal solution of the continuous relaxation, taking the good components from the nodes features matrix in the bipartite-graph representation.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.deviationFrom-Tuple{Any, LearningPi.zero_deviation}","page":"API reference","title":"LearningPi.deviationFrom","text":"function deviationFrom(x,::zerodeviation)\n\n#Arguments:\n\nx: the bipartite-graph representation of the instance\n\nFor the zero_deviation it returns an all-zeros vector with the correct size. The size will be the same as the dual variables associated to the dualized constraints in the optimal solution of the continuous relaxation, taking the good components from the nodes features matrix in the bipartite-graph representation.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.features_matrix-Tuple{Instances.instanceGA, Any, LearningPi.abstract_features_matrix}","page":"API reference","title":"LearningPi.features_matrix","text":"function featuresmatrix(ins::instanceGA,featObj, fmt::abstractfeatures_matrix)\n\n#Arguments:\n\nins: instance structure, it should be a sub-type of instanceGA\nfeatObj: features object containing all the characteristics \nfmt: features matrix type.\n\nConstruct the matrix of the features for a bipartite-graph representation of the instance.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.features_matrix-Tuple{abstractInstanceMCND, LearningPi.featuresMCND, LearningPi.abstract_features_matrix}","page":"API reference","title":"LearningPi.features_matrix","text":"function featuresmatrix(ins::instanceMCND, featObj::featuresMCND, fmt::abstractfeatures_matrix)\n\n#Arguments:\n\nins: instance structure, it should be a sub-type of abstractInstanceMCND\nfeatObj: features object containing all the characteristics \nfmt: features matrix type.\n\nConstruct the matrix of the features for a bipartite-graph representation of the instance.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.features_matrix-Tuple{instanceCWL, Any, LearningPi.abstract_features_matrix}","page":"API reference","title":"LearningPi.features_matrix","text":"function featuresmatrix(ins::instanceCWL,featObj, fmt::abstractfeatures_matrix)\n\n#Arguments:\n\nins: instance structure, it should be a sub-type of instanceCWL\nfeatObj: features object containing all the characteristics \nfmt: features matrix type.\n\nConstruct the matrix of the features for a bipartite-graph representation of the instance.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.gap-Tuple{LearningPi.abstract_example, Real, Real, Real}","page":"API reference","title":"LearningPi.gap","text":"function gap(example::abstract_example, objPred::Real, objGold::Real, nInst::Real)\n\n#Arguments:\n\nexample: the current example (dataset point).\nobjPred: the current obective for the example.\nobjGold: the optimal value of the Lagrangian Dual.\nnInst: the number of the instances in the set.\n\ncompute the GAP of the instance in the example using the predicted objective objPred.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.gap_closure-Tuple{LearningPi.abstract_example, Real, Real, Real}","page":"API reference","title":"LearningPi.gap_closure","text":"function gap(example::abstract_example, objPred::Real, objGold::Real, nInst::Real)\n\n#Arguments:\n\nexample: the current example (dataset point).\nobjPred: the current obective for the example.\nobjGold: the optimal value of the Lagrangian Dual.\nnInst: the number of the instances in the set.\n\ncompute the closure GAP of the instance in the example using the predicted objective objPred. The closure is w.r.t. the value of the Lagrangian Sub-Problem, solved with the dual variables of the continuous relaxation.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.get_cr_features-Tuple{LearningPi.cr_features_matrix, Any, Any, Any}","page":"API reference","title":"LearningPi.get_cr_features","text":"function crfeatures(fmt::crfeatures_matrix, x, y)\n\n#Arguments: -fmt: feature matrix type (it shoul be crfeaturesmatrix) \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.get_cr_features-Tuple{LearningPi.lr_features_matrix, Any, Any, Any}","page":"API reference","title":"LearningPi.get_cr_features","text":"function crfeatures(fmt::lrfeatures_matrix, x, y)\n\n#Arguments: -fmt: feature matrix type (it shoul be lrfeaturesmatrix) \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.get_cr_features-Tuple{LearningPi.without_cr_features_matrix, Any, Any, Any}","page":"API reference","title":"LearningPi.get_cr_features","text":"function crfeatures(fmt::withoutcrfeaturesmatrix, _, _)\n\n#Arguments: -fmt: feature matrix type (it shoul be withoutcrfeatures_matrix) \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.get_device-Tuple{LearningPi.abstract_loss}","page":"API reference","title":"LearningPi.get_device","text":"function getdevice(los::abstractloss)\n\n#Arguments:\n- `_`: the loss parameters\n\nreturns the device (cpu/gpu) used to compute the loss.\nFor a general loss will be CPU.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.get_model-Tuple{LearningPi.Graphormer}","page":"API reference","title":"LearningPi.get_model","text":"get_model(nn::GNNChain)\n\n#Arguments: \t-nn: neural network model\n\nreturns a cpu version of the model that can be saved using a bson file.    \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.get_parameters","page":"API reference","title":"LearningPi.get_parameters","text":"function get_parameters(nn, lt::learningType, f = 0)\n\nnn : neural network\nlt : learning type\nf :  ???\n\nReturns the model parameters of nn in the case in which nn belsong to lt learning type.\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningPi.get_λ-Tuple{Any}","page":"API reference","title":"LearningPi.get_λ","text":"function get_λ(x)\n\n#Arguments:\n\nx: the bipartite-graph representation of the instance\n\nReturns the dual variables associated to the dualized constraints in the optimal solution of the continuous relaxation, taking the good components from the nodes features matrix in the bipartite-graph representation.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.gradient_lsp-Tuple{AbstractVecOrMat, Instances.cpuInstanceGA}","page":"API reference","title":"LearningPi.gradient_lsp","text":"function gradient_lsp( x, ins::cpuInstanceGA)\n\n#Arguments:\n- `x`: the solution of the Lagrangian Sub-problem\n- `ins`: a cpuInstanceCWL structure\n\nThis function compute and returns the gradient of the sub-problem objective function w.r.t. the Lagrangian Multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.gradient_lsp-Tuple{AbstractVecOrMat, cpuInstanceCWL}","page":"API reference","title":"LearningPi.gradient_lsp","text":"function gradient_lsp( x, ins::cpuInstanceCWL)\n\n#Arguments:\n- `x`: the solution of the Lagrangian Sub-problem\n- `ins`: a cpuInstanceCWL structure\n\nThis function compute and returns the gradient of the sub-problem objective function w.r.t. the Lagrangian Multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.gradient_lsp-Tuple{AbstractVecOrMat, cpuInstanceMCND}","page":"API reference","title":"LearningPi.gradient_lsp","text":"function gradient_lsp( x, ins::cpuInstanceMCND)\n\n#Arguments:\n- `x`: the solution of the Lagrangian Sub-problem\n- `ins`: a cpuInstanceMCND structure\n\nThis function compute and returns the gradient of the sub-problem objective function w.r.t. the Lagrangian Multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.printBests-Tuple{Dict, String}","page":"API reference","title":"LearningPi.printBests","text":"function printBests(bestMetrics::Dict, path::String)\n\nArguments:\n\nbestMetrics: a dictionary of float that contains the best values\n\n\t\t   find in the training for altypel the considered metrics.\n\npath: location where print the results in a file.\t\t\t\t\n\nTakes as input the dictionary of the best metrics and print the values in standard output and in a file defined by the path.                \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.printMetrics-Tuple{Dict}","page":"API reference","title":"LearningPi.printMetrics","text":"function printMetrics(currentMetrics::Dict)\n\nArguments:\n\ncurrentMetrics: a dictionary of Float\n\nThis function takes as input the dictionary of the metrics and print the values associated to training and validation sets.    \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.print_best_models-Tuple{String, Dict}","page":"API reference","title":"LearningPi.print_best_models","text":"function printbestmodels(endString::String, bestModels::Dict)\n\n#Arguments:\n\nendString: Path where save the models \nbestModels: Dictionary of the best models (w.r.t different metrics) found so far\n\nPrint in a file BSON, located in the folder endString the best model found so far \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.print_json-Tuple{Any, Any, Any, Any, cpuMCNDinstanceFactory}","page":"API reference","title":"LearningPi.print_json","text":"function print_json(ins::cpuInstanceMCND, lab, feat, fileName,factory::cpuMCNDinstanceFactory)\n\n#Arguments:    \n\nins: instance structure, it should be of type CWLinstance\nlab: labels structure, it should be of type labelsMCND\nfeat: features structure, it should be of type featuresMCND\nfileName: the path to the file json where print the data\nfactory: instance factory should be of type cpuMCNDinstanceFactory\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.print_json-Tuple{Instances.instanceGA, LearningPi.labelsGA, LearningPi.featuresGA, String}","page":"API reference","title":"LearningPi.print_json","text":"function print_json(ins::instanceGA, lab::labelsGA, feat::featuresGA, fileName::String)\n\n#Arguments:     -ins: instance structure, it should be of type <: instanceGA\n\nlab: labels structure, it should be of type labelsGA\nfeat: features structure, it should be of type featuresGA\nfileName: the path to the file json where print the data\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.print_json-Tuple{instanceCWL, LearningPi.labelsCWL, LearningPi.featuresCWL, String}","page":"API reference","title":"LearningPi.print_json","text":"function print_json(ins::instanceCWL, lab::labelsCWL, feat::featuresCWL, fileName::String)\n\n#Arguments:     -ins: instance structure, it should be of type <: instanceCWL\n\nlab: labels structure, it should be of type labelsCWL\nfeat: features structure, it should be of type featuresCWL\nfileName: the path to the file json where print the data\n\nPrint the information provided in the instance ins, the labels lab and the features feat in a JSON file located in the path fileName.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.read_labels-Tuple{String, Instances.instanceGA}","page":"API reference","title":"LearningPi.read_labels","text":"function read_labels(fileLabel::String, ins::instanceGA)\n\n#Arguments:\n\t- `fileLabel`: the path to the file where to find labels informations\n\t- `ins`: instance object, it should be sub-type of instanceGA \n\nread the labels and returns a labels structure.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.read_labels-Tuple{String, abstractInstanceMCND}","page":"API reference","title":"LearningPi.read_labels","text":"function read_labels(fileLabel::String, ins::abstractInstanceMCND)\n\n#Arguments:\n\t- `fileLabel`: the path to the file where to find labels informations\n\t- `ins`: instance object, it should be of type abstractInstanceMCND \n\nread the labels and returns a labels structure.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.read_labels-Tuple{String, instanceCWL}","page":"API reference","title":"LearningPi.read_labels","text":"function read_labels(fileLabel::String, ins::instanceCWL)\n\n#Arguments:\n\t- `fileLabel`: the path to the file where to find labels informations\n\t- `ins`: instance object, it should be of type sub-type of instanceCWL \n\nread the labels and returns a labels structure.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.saveHP-Tuple{String, Float32, Float32, Vector{Int64}, Any, LearningPi.learningGNN, LearningPi.abstract_features_matrix, LearningPi.abstract_deviation, Any, Int64, Int64, Vararg{Any, 7}}","page":"API reference","title":"LearningPi.saveHP","text":"function saveHP(endString::String,lr::Float32,decay::Float32,h::Vector{Int64},opt,lt::learningType,loss,seedNN::Int64,seedDS::Int64,stepSize::Int64)\n\nArguments:\n\nendString: a string used as name for the output file.\nlr: learning rate of the algorithm.\ndecay: decay for the learning rate.\nh: a list of #(hidden layers), each component of h contains the number of nodes in\n\n the associated hidden layer.\n\nopt: optimizer\nlt: learning type object.     \nfmt:\ndt:\nloss: loss function.\nseedDS: random seed for the dataset generation.\nseedNN: random seed for the neural network parameters.\nstepSize: the step size for the decay scheduler of the optimizer\nnodes_number: size (number of nodes) in the hidden reprensentation between each layer\nblock_number: number of blocks in the model\nhI: sizes of Dense layers in the first part, where the nodes features are sent in the hidden space\nhF: sizes of Dense layers in the final\ndataPath: path to the instances used in the dataset\nfactory: instance factory type\n\nThis function memorize all this hyper parameters in a JSON file.     \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.saveHP-Tuple{String, Float32, Float32, Vector{Int64}, Any, LearningPi.learningType, Any, Int64, Int64, Any}","page":"API reference","title":"LearningPi.saveHP","text":"function saveHP(endString::String,lr::Float32,decay::Float32,h::Vector{Int64}, opt, lt::learningType, loss,seedNN::Int64, seedDS::Int64, stepSize::Int64)\n\nArguments:\n\nendString: a string used as name for the output file.\nlr: learning rate of the algorithm.\ndecay: decay for the learning rate.\nh: a list of #(hidden layers), each component of h contains the number of nodes in\n\n the associated hidden layer.\n\nopt: optimizer\nlt: learning type object.     \nloss: loss function.\nseedDS: random seed for the dataset generation.\nseedNN: random seed for the neural network parameters.\nstepSize: the step size for the decay scheduler of the optimizer\n\nThis function memorize all this hyper parameters in a JSON file.     \n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.sizeFeatures-Tuple{LearningPi.learningMLP, Any}","page":"API reference","title":"LearningPi.sizeFeatures","text":"sizeFeatures(lt::learningType,dS)\n\nArguments:\n\nlt : learning type (general).\ndS : dataset (corpus structure)\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.size_features_constraint-Tuple{LearningPi.cr_features_matrix}","page":"API reference","title":"LearningPi.size_features_constraint","text":"function sizefeaturesconstraint(fmt::crfeaturesmatrix)\n\n#Arguments: -fmt: feature matrix type (it shoul be crfeaturesmatrix) \n\nreturns the size of the features associated to the constraints. In this case 6.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.size_features_constraint-Tuple{LearningPi.lr_features_matrix}","page":"API reference","title":"LearningPi.size_features_constraint","text":"function sizefeaturesconstraint(fmt::lrfeaturesmatrix)\n\n#Arguments: -fmt: feature matrix type (it shoul be crfeaturesmatrix) \n\nreturns the size of the features associated to the constraints. In this case 6.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.size_features_constraint-Tuple{LearningPi.without_cr_features_matrix}","page":"API reference","title":"LearningPi.size_features_constraint","text":"function sizefeaturesconstraint(fmt::withoutcrfeatures_matrix)\n\n#Arguments: -fmt: feature matrix type (it shoul be crfeaturesmatrix) \n\nreturns the size of the features associated to the constraints. In this case 4.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.size_features_variable-Tuple{LearningPi.cr_features_matrix}","page":"API reference","title":"LearningPi.size_features_variable","text":"function sizefeaturesvariable(fmt::crfeaturesmatrix)\n\n#Arguments: -fmt: feature matrix type (it shoul be crfeaturesmatrix) \n\nreturns the size of the features associated to the variables. In this case 4.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.size_features_variable-Tuple{LearningPi.lr_features_matrix}","page":"API reference","title":"LearningPi.size_features_variable","text":"function sizefeaturesvariable(fmt::lrfeaturesmatrix)\n\n#Arguments: -fmt: feature matrix type (it shoul be crfeaturesmatrix) \n\nreturns the size of the features associated to the variables. In this case 4.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.size_features_variable-Tuple{LearningPi.without_cr_features_matrix}","page":"API reference","title":"LearningPi.size_features_variable","text":"function sizefeaturesvariable(fmt::withoutcrfeatures_matrix)\n\n#Arguments: -fmt: feature matrix type (it shoul be crfeaturesmatrix) \n\nreturns the size of the features associated to the variables. In this case 2.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.sub_problem_value-Tuple{Any, Any, Any, LearningPi.abstract_loss}","page":"API reference","title":"LearningPi.sub_problem_value","text":"function subproblemvalue(, v, example, _::abstractloss)\n\n#Arguments:\n\n_: lagrangian multipliers vector candidate, \nv: the value of the loss function,\nexample: dataset sample object,\n_: loss parameters.\n\nCompute the value of the sub-problem for the loss for which it cannot be obtained in a smarter way.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.sub_problem_value-Tuple{Any, Any, Any, LearningPi.loss_hinge}","page":"API reference","title":"LearningPi.sub_problem_value","text":"function subproblemvalue(_, v, example, _::HingeLoss)\n\n#Arguments:\n\n_: lagrangian multipliers vector candidate, \nv: the value of the loss function,\nexample: dataset sample object,\n_: loss parameters, it should be a structure of type HingeLoss.   \n\nCompute the value of the sub-problem without recomputing it, but using the value of the loss function (for the HingeLoss)  and other informations contained in the sample\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.testAndPrint-Tuple{Dict, Any, Any, Any, LearningPi.learningType, LearningPi.abstract_deviation}","page":"API reference","title":"LearningPi.testAndPrint","text":"function testAndPrint(currentMetrics::Dict,testSet,nn,loss,loss,lt::learningType)\n\nArguments:\n\ncurrentMetrics: a dictionary of Float \ntestSet : a vector of hingeExamples_instance that correspond to the test set.\nnn : a neural network model.\nloss: a structure with the parameters of the loss.\n\n\t\t   For other details of the parameters of a certain loss see the definition of the particular structure of the loss.\n\nlt: learning type object.     \n\nThis function compute different metrics over the validation set. The values are memorized in the dictionary and print them in the standard output.\n\n\n\n\n\n","category":"method"},{"location":"api/#LearningPi.validation-Tuple{Dict, Any, Any, Any, LearningPi.learningType, LearningPi.abstract_deviation}","page":"API reference","title":"LearningPi.validation","text":"function validation(currentMetrics::Dict, valSet, nn,loss::abstractLoss,  lt::learningType,dt::abstract_deviation)\n\nArguments:\n\ncurrentMetrics: a dictionary of Float \nvalSet : a vector of gnn_dataset that correspond to the validation set.\nnn : a neural network model.\nloss: a structure with the parameters of the loss.\n\n\t   For other details of the parameters of a certain loss see the definition of the particular structure of the loss.\n\nloss: loss function.\nlt: learning type object.       \ndt: deviation type (0 or dual of the CR)        \n\nThis function compute different metrics over the validation set. The values are memorized in the dictionary.\n\n\n\n\n\n","category":"method"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"This page is only devoted to describe how easily launch the training and the test.","category":"page"},{"location":"Training/#Training","page":"Training Scripts","title":"Training","text":"","category":"section"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"The training the GNN models presented in the paper can be performed using the script in runKfoldTransformer.jl","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"For example, assuming that we are in the main directory of the project, with the command:","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"julia --project=. ../../src/runKfoldTransformer.jl --lr 0.0001 --seed 1 --decay 0.9 --opt rADAM --MLP_i 250 --MLP_h 1000 --MLP_f 250 --lossType LRloss --lossParam 0.9 --maxEp 300 --stepSize 1000000 --kFold 1 --data /users/demelas/MCNDsmallCom40/ --block_number 5 --nodes_number 500 --pDrop 0.25 --factory cpuMCNDinstanceFactory --learningType learningSampleTransformer --cr_deviation true --cr_features true","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"we train one learningSampleTransformer (learningType) model composed by 5 blocks (block_number) and an hidden state representation of size 500 (--nodes_number). The first MLP that goes from the features space to the hidden representation is composed of only one hidden layer with 250 nodes (--MLP_i). Also the Decoder has only one hidden layer of size 250  (--MLP_f). Instead the hidden MLP always goes from the hidden state representation to a space 2 times bigger (--MLP_h). The drop-out probability is setted to 0.25 (--pDrop).","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"The training is performed on the dataset composed by cpuMCNDinstanceFactory (--factory) that can be found in the path /users/demelas/MCNDsmallCom40/ (--data). To construct the dataset we use the seed 1 (--seed) and it will be the same seed used to construct and initialize the model. When the dataset is divised in folds (using the provided seed) the fold 1 is selected as test set ('–kFold`). Notice that even if we change only the seed, but keep the same k-fold, actually the training, validation and test will not be the same.","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"The Optimizer used for the training is rADAM (--opt) with learning rate 0.0001 (--lr) and decay 0.9 (--decay). The step size for the decay (i.e. number of samples before update the learning rate) is 1000000.","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"The model, tensorboard logs and other information about the training/validation/test sets and the hyper-parameters choices are saved in a sub-directory of the directory run that will be created in the directory where we launch the training.","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"To launch the experiments we use slurm file of the type. ","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"#!/usr/bin/env bash\n#SBATCH --job-name=T-1-5-s0\n#SBATCH --nodes=1 \n#SBATCH --ntasks-per-node=1    \n#SBATCH --cpus-per-task=1     \n#SBATCH --ntasks=1 \n#SBATCH --gres=gpu:1       \n#SBATCH --qos=qos_gpu_t4        \n#SBATCH --output=output.txt    \n#SBATCH --error=error.txt \n\nexport JULIA_NUM_THREADS=1\njulia --project=../.. ../../src/runKfoldTransformer.jl --lr 0.0001 --seed 1 --decay 0.9 --opt ADAM --MLP_i 250 --MLP_h 1000 --MLP_f 250 --lossType LRloss --lossParam 0.9 --maxEp 300 --stepSize 1000000 --kFold 1 --data /users/demelas/MCNDsmallCom40/ --block_number 5 --nodes_number 500 --pDrop 0.25 --factory cpuMCNDinstanceFactory --learningType learningSampleTransformer --cr_deviation true --cr_features true","category":"page"},{"location":"Training/#Testing","page":"Training Scripts","title":"Testing","text":"","category":"section"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"Documentation Work in Progress...","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LearningPi","category":"page"},{"location":"#LearningPi","page":"Home","title":"LearningPi","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LearningPi.","category":"page"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"In this section we will provided further details about the Sampling Mechanism implemented in the Package.","category":"page"},{"location":"Sampling/#Sampling-Type","page":"Sampling Mechanism","title":"Sampling Type","text":"","category":"section"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"The package dispose of only one type of Sampling.","category":"page"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"Anyway this sampling can be enbedded into the models in three differents ways, providing different architectures:","category":"page"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"In the hidden space representation as [1], before use the decoder.\nIn all the hidden space representations. As the previous the sampling is performed in the hidden space, the difference is that the sampling in this case is performed between all the blocks and not only before the decoder. \nIn the output space, in this case the decoder returns one vector of size two for each dualized constraint representing the mean and the standard deviation of a Gaussian distribution that directly sample in the Lagrangian Multipliers Space.","category":"page"},{"location":"Sampling/#References","page":"Sampling Mechanism","title":"References","text":"","category":"section"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"[1]: F. Demelas, J. Le Roux, M. Lacroix, A. Parmentier \"Predicting Lagrangian Multipliers for Mixed Integer Linear Programs\", ICML 2024.","category":"page"},{"location":"Models/#General-Models","page":"Machine Learning Models","title":"General Models","text":"","category":"section"},{"location":"Models/#MLP","page":"Machine Learning Models","title":"MLP","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"A Classic Multi Layer Perceptron that receeives as input one features vector (for each dualized constraint) and predict (in parallel) one Lagrangian Multiplier Value (for each relaxed constraint).","category":"page"},{"location":"Models/#GNN-Models","page":"Machine Learning Models","title":"GNN Models","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"The key models of this project belongs from this family and can be all seen as particular instantiation of the more general model implemented in Graphormer.jl.","category":"page"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"The key component of this model consists on the block presented in [1].","category":"page"},{"location":"Models/#Some-Easy-to-Repeat-Models","page":"Machine Learning Models","title":"Some Easy-to-Repeat Models","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"In this section we will present some GNN models that are already implemented as partticular instantiation of the model defined in Graphormer.jl.","category":"page"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"All the constructors for this models can be found in the file ModelFactory.jl.","category":"page"},{"location":"Models/#LearningTransformer","page":"Machine Learning Models","title":"LearningTransformer","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"Simply consists on sequential Chain of the block presented in [1], without Sampling Mechanism.","category":"page"},{"location":"Models/#LearningSampleTransformer","page":"Machine Learning Models","title":"LearningSampleTransformer","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"As LearningTransformer, the only difference is that for this model we consider the Sampling mechanism, as presented in [1]. ","category":"page"},{"location":"Models/#LearningSampleGasse","page":"Machine Learning Models","title":"LearningSampleGasse","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"As LearningSampleTransformer this architecture consider the same Sampling Mechanism presented in [1]. Insted of using ours architecture it use one more near to the one presented by Gasse et al. in [2].","category":"page"},{"location":"Models/#LearningSampleNair","page":"Machine Learning Models","title":"LearningSampleNair","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"As LearningSampleTransformer this architecture consider the same Sampling Mechanism presented in [1]. Insted of using ours architecture it use one more near to the one presented by Nair et al. in [3]","category":"page"},{"location":"Models/#LearningSampleOutside","page":"Machine Learning Models","title":"LearningSampleOutside","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"As LearningTransformer, the only difference is that for this model we consider a sampling mechanism. While LearningSampleTransformer sample in the hidden space (as presented in [1]), in this case we sample directly in the output space. More details on the sampling mechanism can be found in the apposite Section. ","category":"page"},{"location":"Models/#LearningMultiPredTransformer","page":"Machine Learning Models","title":"LearningMultiPredTransformer","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"This model as the same inner structure as LearningTransformer, but it contains several decoders and so is able to provide several Lagrangian Multipliers prediction using the same model (maximum one for block). The model LearningTransformer can be seen as this model with only one decoder at the end of the Block Chain. No sample mechanism is used in this case.","category":"page"},{"location":"Models/#LearningMultiPredSample","page":"Machine Learning Models","title":"LearningMultiPredSample","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"This model as the same inner structure as LearningSampleTransformer, but it contains several decoders and so is able to provide several Lagrangian Multipliers prediction using the same model (maximum one for block). The model LearningSampleTransformer can be seen as this model with only one decoder at the end of the Block Chain. The sampling mechanism is the same as LearningSampleTransformer for each predicted Lagrangian Multipliers vector.","category":"page"},{"location":"Models/#References:","page":"Machine Learning Models","title":"References:","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"[1]: F. Demelas, J. Le Roux, M. Lacroix, A. Parmentier \"Predicting Lagrangian Multipliers for Mixed Integer Linear Programs\", ICML 2024.","category":"page"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"[2]: Gasse, M., Chételat, D., Ferroni, N., Charlin, L., and Lodi, A. Exact Combinatorial Optimization with Graph Convolutional Neural Networks. In Wallach, H., Larochelle, H., Beygelzimer, A., Alché-Buc, F. d., Fox, E., and Garnett,R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.","category":"page"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"[3]: Nair, V., Bartunov, S., Gimeno, F., von Glehn, I., Lichocki, P., Lobov, I., O’Donoghue, B., Sonnerat, N., Tjandraatmadja, C., Wang, P., Addanki, R., Hapuarachchi, T., Keck, T., Keeling, J., Kohli, P., Ktena, I., Li, Y., Vinyals, O., and Zwols, Y. Solving mixed integer programs using neural networks. CoRR, abs/2012.13349, 2020.","category":"page"}]
}
