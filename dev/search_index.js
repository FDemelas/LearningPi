var documenterSearchIndex = {"docs":
[{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"In this package are developped different loss functions:","category":"page"},{"location":"Loss/#Lagrangian-Sub-Problem-Loss","page":"Loss Functions","title":"Lagrangian Sub-Problem Loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_LR is the one presented in the paper [1] and consist on the bound provided by the Lagrangian Sub-Problem (with a proper sign that allows to write the Lagrangian Dual as minimization problem).","category":"page"},{"location":"Loss/#Lagrangian-Sub-Problem-Loss-on-GPU","page":"Loss Functions","title":"Lagrangian Sub-Problem Loss on GPU","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_LR_gpu: as LRloss but the sub-problem is solved in GPU.","category":"page"},{"location":"Loss/#Warning:-for-the-moment-this-loss-function-works-only-for-MCND-instances-with-GPU-memorization.","page":"Loss Functions","title":"Warning: for the moment this loss function works only for MCND instances with GPU memorization.","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"Anyway it is not faster than the one that use CPU.","category":"page"},{"location":"Loss/#GAP-Loss","page":"Loss Functions","title":"GAP Loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_GAP this loss function simply consists on the GAP of percentage  the value v provided by the Lagrangian sub-Problem and the optimal value of the Lagrangian dual and can be computed as:","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"fracv-v^*v^**100","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"when the Lagrangian Dual is a minimization problem","category":"page"},{"location":"Loss/#GAP-Closure-Loss","page":"Loss Functions","title":"GAP Closure Loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_GAP_closure this loss function is similar to GAPloss as still consider the value v provided by the Lagrangian sub-Problem and the optimal value of the Lagrangian dual. But it tries to further compare these solutions with the continuous relaxation bound CR. It is computed as:","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"fracvv^*-CR * 100","category":"page"},{"location":"Loss/#Hinge-Loss","page":"Loss Functions","title":"Hinge Loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"For an instance iota in I with gold solution (x^* y^*) (more precisely (x^*(iota) y^*(iota))) of L(pi^*), the Hinge loss is","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"H(wiota) =  L(pi(w) x^* y^*) - min_xy Big(L(pi(w) x y) - alpha Delta_y^*(y)Big)","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"where w are the parameters of the model, pi(w) is the prediction of the model given w, (x^*y^*)  is the gold solution of the instance,  Delta_y^*(y) is the hamming loss between y and y^* and alpha is a non negative scalar.","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"The loss_hinge is ","category":"page"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"frac1Isum_iota in I frac1A(iota)H(wiota)","category":"page"},{"location":"Loss/#Mean-Squared-Error","page":"Loss Functions","title":"Mean Squared Error","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_mse is only the MSE between the mean squared error between the predicted and the optimal Lagrangian Multipliers.","category":"page"},{"location":"Loss/#Multi-Prediction-LR-loss","page":"Loss Functions","title":"Multi Prediction LR loss","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"loss_multi_LR_factory is a specialized version of loss_LR that is able to handle with Multiple Lagrangian Multipliers Predictions.","category":"page"},{"location":"Loss/#More-sophisticated-variants-of-this-loss-function-will-be-provided-in-the-future.","page":"Loss Functions","title":"More sophisticated variants of this loss function will be provided in the future.","text":"","category":"section"},{"location":"Loss/#References","page":"Loss Functions","title":"References","text":"","category":"section"},{"location":"Loss/","page":"Loss Functions","title":"Loss Functions","text":"[1]: F. Demelas, J. Le Roux, M. Lacroix, A. Parmentier \"Predicting Lagrangian Multipliers for Mixed Integer Linear Programs\", ICML 2024.","category":"page"},{"location":"api_private/#Private-API-for-LearningPi.jl","page":"Private APIs","title":"Private API for LearningPi.jl","text":"","category":"section"},{"location":"api_private/","page":"Private APIs","title":"Private APIs","text":"Modules = [LearningPi]\nPublic= false","category":"page"},{"location":"api_private/#LearningPi.AbstractModel","page":"Private APIs","title":"LearningPi.AbstractModel","text":"Abstract type for the Neural Networks models.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.AbstractSampler","page":"Private APIs","title":"LearningPi.AbstractSampler","text":"Abstract type for the Sampling functions.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.Corpus","page":"Private APIs","title":"LearningPi.Corpus","text":"Corpus structure contains the three datasets: training, validation and test set\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.Graphormer","page":"Private APIs","title":"LearningPi.Graphormer","text":"This structure provide an implementation of the main neural-network model of this project.\n\nFields:\n\nHiddenMap: a Chain to map the features into the Hidden Space,\nGraphormers: a GNNChain to apply several Graphormer Blocks to the hidden-features representation, each component can be seen as main-block of the model,\nDecoders: a Chain of decoders, it should have the same size as the desired predictions,\nSampling: sampling function,\nn_gr: number of main-blocks,\ntrain_mode: if the model is in training mode or not, the main change is that if off we does not sample, but just take the mean,\nprediction_layers: indexes of the main-blocks after which we want insert a Decoder to provide a Lagrangian Multipliers Prediction,\nwhere_sample: a SamplingPosition to handle different possibilities of Sampling,\nonly_last: a boolean that says if we want only a single Lagragian Multipliers prediction associated to the last main-block,\ndt: deviation type.\n\nThe constructor of this structure have the following \n\nArguments:\n\nHiddenMap: as in the Fields,\nGraphormers: as in the Fields,\nDecoders: as in the Fields,\nSampling:as in the Fields,\nwhere_sample:as in the Fields,\nprediction_layers: as in the Fields, by default empty, in this case we predict only in the last Graphormers layers,\ndt: as in the Fields, by default cr_deviation.\n\nThis structure is declared as Flux.functor in order to efficiently and automatically implement the back-propagation. It can be called providing as input simply the graph-neural-network   (a GNNGraph). \n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.Graphormer-Tuple{Any}","page":"Private APIs","title":"LearningPi.Graphormer","text":"Arguments:\n\nx: input of the NN model of type Graphormer  (a GNNGraph).  \n\nForward computation of a Graphormer m, the output is the concatenation of all the multipliers predicted by the model   \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.GraphormerBlock","page":"Private APIs","title":"LearningPi.GraphormerBlock","text":"Structure that implement the basic main machine-learning block of this .\n\nFields:\n\nConvolution: a Graph Convolutional Neural Network that performs one graph-message passing,\nMLP: a Multi-Layer-Perceptron that implement the non-linear part in parallel over all the node-hidden-features. \n\nThe first constructor takes as input the following\n\nArguments:\n\nhidden_sample: a structure composed by three boolean fields to handle the sampling positions in the  model,\ninpOut: the size  of the hidden space, \ninit: initialization for the parameters of the models,\nrng: random number generator for the sampler, dropout, and all the other random components of the model,\npDrop: dropout probability,\nh_MLP: a vector containing at each component the number of nodes in the associated layer of the hidden multi-layer-perceptron,\nConvLayer: convolutional layer, by default is GraphConv,\nact: activation function for the Multi-Layer-Perceptron, by default is relu,\nact_conv: activation function for the Graph Convolutional Part, by default is  identity,\naggr: aggregation function for the Graph Convolutional Part, by default is mean.\n\nThe second constructor directly takes as input the Fields of the structure.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.GraphormerBlock-Tuple{Any, Any}","page":"Private APIs","title":"LearningPi.GraphormerBlock","text":"Arguments:\n\nx: a GNNGraph,\nh: a features matrix associated to the nodes of x.\n\nComputes the forward for the GraphormerBlock. The backward is automatically computed as long as all the operation in the forward are differentiable by Zygote.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.LayerNorm","page":"Private APIs","title":"LearningPi.LayerNorm","text":"Fields:\n\neps: regularization parameter,\nd: size of the input of the normalization layer. \n\nDescribe the Layer Normalization for the provided parameters\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.LayerNorm-Tuple{Any}","page":"Private APIs","title":"LearningPi.LayerNorm","text":"Perform a Layer Normalization using x as input .\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.RMSNorm","page":"Private APIs","title":"LearningPi.RMSNorm","text":"Fields:\n\neps: additive regularization parameter,\nsqrtd: multiplicative regularization parameter.\n\nDescribe the RMS Normalization for the provided parameters.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.Sampler","page":"Private APIs","title":"LearningPi.Sampler","text":"Structure that implement the Sampling mechanism from a Gaussian distribution.\n\nFields:\n\nrng: random number generator.\n\nAn instantiation of this structure can be used as function. \n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.Sampler-Tuple{Any}","page":"Private APIs","title":"LearningPi.Sampler","text":"Arguments:\n\nx a vector (of length even), the first half components are the mean μ and the last hals the standard deviation σ.\n\nThe standard deviation is bounded in [-6,2] ... magic numbers. The output is a vector of size half the size of x sampled from a gaussian of mean μ and standard deviation σ. \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.SamplingPosition","page":"Private APIs","title":"LearningPi.SamplingPosition","text":"Structure that handle the position in the model where is performed the sample. For the moment only three alternative are available and are encoded in boolean fields.\n\nFields:\n\noutside: if true the sampling is performed in the output space,\nhidden_state: in all the hidden states between two main blocks,\nbefore_decoding: in the hidden space, but only before call the decoder. \n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.abstract_dataset","page":"Private APIs","title":"LearningPi.abstract_dataset","text":"Abstract type for the dataset\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.abstract_deviation","page":"Private APIs","title":"LearningPi.abstract_deviation","text":"Abstract type to handle the deviation vector, i.e. the starting point from which our model produce an additive activation. \n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.abstract_example","page":"Private APIs","title":"LearningPi.abstract_example","text":"Abstract type for an element of a dataset\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.abstract_features_matrix","page":"Private APIs","title":"LearningPi.abstract_features_matrix","text":"Abstract type for the construction of the nodes-features matrix associated to the bipartite graph representation of the instance.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.cr_deviation","page":"Private APIs","title":"LearningPi.cr_deviation","text":"Type to use as deviation vector (i.e. the starting point from which our model produce an additive activation) the dual variables associated to the relaxed constraints in the optimal solution of the Continuous Relaxation. \n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.cr_features_matrix","page":"Private APIs","title":"LearningPi.cr_features_matrix","text":"with this choice the features matrix considers the informations related to the continuous relaxation of the instance.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.example_gnn","page":"Private APIs","title":"LearningPi.example_gnn","text":"Structure to encode the examples for the training when we whant use a GNN model.\n\nFields:\n\ninstance: an instance,\nfeatures: the features associated to the instance,\t\ngold: the labels,\nlinear_relaxation: the Lagrangian Subproblem value associated to the dual variable of the continuous relaxation.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.featuresCWL","page":"Private APIs","title":"LearningPi.featuresCWL","text":"Features structure for the Capacitated Warehouse Location instance.\n\nFields:\n\n-xCR: primal solution of the Linear Relaxation associated to the variables that associate one items to a pack, -yCR: primal solution of the Linear Relaxation associated to the variables say if we use or not a pack, -λ: dual solution of the Linear Relaxation associated to the packing constraints, -μ: dual solution of the Linear Relaxation associated to the packing constraints, -objCR: objective value of the Linear Relaxation, -xLR: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the dual variables λ of the linear relaxation), -yLR: primal solution of the Knapsack Lagrangian Relaxation associated to the variables say if we use or not a pack (using the dual variables λ of the linear relaxation), -objLR: objective value of the Knapsack Lagrangian Relaxation (using the dual variables λ of the linear relaxation). \n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.featuresGA","page":"Private APIs","title":"LearningPi.featuresGA","text":"Features structure for the Generalized Assignment instance.\n\nFields:\n\n-xCR: primal solution of the Linear Relaxation associated to the variables that associate one items to a pack, -λ: dual solution of the Linear Relaxation associated to the packing constraints, -μ: dual solution of the Linear Relaxation associated to the packing constraints, -objCR: objective value of the Linear Relaxation, -xLR: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the dual variables λ of the linear relaxation), -objLR: objective value of the Knapsack Lagrangian Relaxation (using the dual variables λ of the linear relaxation). \n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.featuresMCND","page":"Private APIs","title":"LearningPi.featuresMCND","text":"Struct containing the information of the features for an instance.\n\nFields:\n\nxCR: the value of the flow variables for the optimal solution of the linear relaxation,\nyCR: the value of the decision variables for the optimal solution of the linear relaxation,\nλ: the value of the dual variables associated to the flow constraints, for the optimal solution of the linear relaxation,\nμ: the value of the dual variables associated to the capacity constraints for the optimal solution of the linear relaxation,\nobjCR: the objective value of the linear relaxation,\nxLR: the value of the flow variables for the optimal solution of the sub-problem for the knapsack relaxation, considering as lagrangian multiers the vector λ,\nyLR: the value of the design variables for the optimal solution of the sub-problem for the knapsack relaxation, considering as lagrangian multiers the vector λ,\nLRarcs: the objective values, for each edge, of the optimal solution of the sub-problem for the knapsack relaxation, considering as lagrangian multiers the vector λ,\nobjLR: the objective value of the sub-problem for the knapsack relaxation, considering as lagrangian multiers the vector λ,\norigins: a matrix of size K×V the cost of the shortest path from the origin to the current node with costs in an edge e:  ins.r[k,e]+ins.f[e]/ins.c[e],\ndestinations: a matrix of size K×V the cost of the shortest path from the current node to the destination with costs in an edge e:  ins.r[k,e]+ins.f[e]/ins.c[e],\ndistance: a matrix of size V×V with the distance in terms of number of edges for the shortest path from each two nodes.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.gnn_dataset","page":"Private APIs","title":"LearningPi.gnn_dataset","text":"Structure to encode the dataset composed by example_gnn.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.labels","page":"Private APIs","title":"LearningPi.labels","text":"Struct containing the information relative to the labels\n\nFields:\n\nπ: matrix containing the gold Lagrangian multipliers. π[k, i] gives the values of the Lagrangian multiplier associated with demand k and node i,\nx: solution x of the Lagrangian problem. x[k, a] gives the value of the solution x_a^k of the Lagrangian problem L(π) for demand k and arc a,\ny: solution y of the Lagrangian problem. y[a] gives the value of the solution y_a of the Lagrangian problem L(π) for arc a,\nLRarcs: Values of the Lagrangian problem for the arcs. LRarcs[a] gives the value of subproblem L_a associated with arc a,\nobjLR: Value of the Lagrangian dual problem.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.labelsCWL","page":"Private APIs","title":"LearningPi.labelsCWL","text":"Label structure for the Capacitated Warehouse Location Problem.\n\nFields:\n\n-`π`: optimal lagrangian multipliers vector,\n-`xLR`: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the optimal Lagrangian multipliers),\n-`yLR`: primal solution of the Knapsack Lagrangian Relaxation associated to the variables say if we use or not a pack (using the optimal Lagrangian multipliers),\n-`objLR`: optimal value of the Lagrangian Dual.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.labelsGA","page":"Private APIs","title":"LearningPi.labelsGA","text":"Label structure for the Generalized Assignment Problem.\n\n# Fields:\n-`π`: optimal lagrangian multipliers vector,\n-`xLR`: primal solution of the Lagrangian Subproblem with optimal Lagrangian multipliers,\n-`objLR`: optimal value of the Lagrangian Dual.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningBlockGNN","page":"Private APIs","title":"LearningPi.learningBlockGNN","text":"Abstract type to type the functions that should work with Graph-Neural-Networks. This abstract type was originally tought to be used for the models that use the Block Architecture here implemented. In the current implementation it coincides more or less to learningGNN.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningGNN","page":"Private APIs","title":"LearningPi.learningGNN","text":"Abstract type to type the functions that should work with Graph-Neural-Networks.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningMLP","page":"Private APIs","title":"LearningPi.learningMLP","text":"Structure that implement the learning type for a simple Multi-Layer-Perceptron (without Graph Neural Network). The features extraction is a simple manual features extraction and the model predict in parallel one value for each dualized constraints.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningMultiPredSample","page":"Private APIs","title":"LearningPi.learningMultiPredSample","text":"Struct to easily construct a neural network architecture similar to learningSampleTransformer, but predict multiple deviation using different decoders enbedded at the end of given main-blocks.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningMultiPredTransformer","page":"Private APIs","title":"LearningPi.learningMultiPredTransformer","text":"Struct to easily construct a neural network architecture similar to learningMultiPresSample, but it does not perform sampling at all.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningSampleGasse","page":"Private APIs","title":"LearningPi.learningSampleGasse","text":"Struct to easily construct a neural network architecture inspired from:\n\nGasse, M., Chételat, D., Ferroni, N., Charlin, L., and Lodi, A. Exact Combinatorial Optimization with Graph Convolutional Neural Networks. In Wallach, H., Larochelle, H., Beygelzimer, A., Alché-Buc, F. d., Fox, E., and Garnett,R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nSubtype of learningBlockGNN.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningSampleNair","page":"Private APIs","title":"LearningPi.learningSampleNair","text":"Struct to easily construct a neural network architecture inspired from:\n\nNair, V., Bartunov, S., Gimeno, F., von Glehn, I., Lichocki, P., Lobov, I., O’Donoghue, B., Sonnerat, N., Tjandraatmadja, C., Wang, P., Addanki, R., Hapuarachchi, T., Keck, T., Keeling, J., Kohli, P., Ktena, I., Li, Y., Vinyals, O., and Zwols, Y. Solving mixed integer programs using neural networks. CoRR, abs/2012.13349, 2020.\n\nSubtype of learningBlockGNN.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningSampleOutside","page":"Private APIs","title":"LearningPi.learningSampleOutside","text":"Struct to easily construct a neural network architecture similar to learningSampleTransformer, but that perform instead the sampling in the lagrangian multipliers output space.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningSampleTransformer","page":"Private APIs","title":"LearningPi.learningSampleTransformer","text":"Struct to easily construct a neural network architecture presented in:\n\nF. Demelas, J. Le Roux, M. Lacroix, A. Parmentier \"Predicting Lagrangian Multipliers for Mixed Integer Linear Programs\", ICML 2024.\n\nSubtype of learningBlockGNN.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningTransformer","page":"Private APIs","title":"LearningPi.learningTransformer","text":"Struct to easily construct a neural network architecture similar to learningSampleTransformer, but it does not perform sampling at all.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.learningType","page":"Private APIs","title":"LearningPi.learningType","text":"Abstract type to type the functions that should work with all the type of models and features encoding\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_GAP","page":"Private APIs","title":"LearningPi.loss_GAP","text":"Structure that realize a GAP loss. This structure can be used as function.\n\nFields:\n\nlr: a lagranian sub-problem loss of type loss_LR.\n\nThe constructor need no paramameters.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_GAP-Tuple{Any}","page":"Private APIs","title":"LearningPi.loss_GAP","text":"Arguments:\n\nπ: a Lagrangian Multipliers Vector,\nexample: an abstract example.\n\nComputes the value of the GAP loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.loss_GAP_closure","page":"Private APIs","title":"LearningPi.loss_GAP_closure","text":"Structure that realize a GAP closure loss. This structure can be used as function.\n\nFields:\n\nlr: a lagranian sub-problem loss of type loss_LR.\n\nThe constructor need no paramameters.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_GAP_closure-Tuple{Any}","page":"Private APIs","title":"LearningPi.loss_GAP_closure","text":"Arguments:\n\nπ: a Laagrangian Multipliers Vector,\nexample: an abstract example.\n\nComputes the value of the GAP closure loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.loss_GAP_closure_factory","page":"Private APIs","title":"LearningPi.loss_GAP_closure_factory","text":"Structure that should be used to construct a GAP closure loss function.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_GAP_factory","page":"Private APIs","title":"LearningPi.loss_GAP_factory","text":"Structure that should be used to construct a GAP loss function.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_LR","page":"Private APIs","title":"LearningPi.loss_LR","text":"Structure that realize a LR (CPU) loss. This structure can be used as function. The constructor need no paramameters.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_LR-Tuple{Any}","page":"Private APIs","title":"LearningPi.loss_LR","text":"Arguments:\n\nπ: a Lagrangian Multipliers Vector,\nexample: an abstract example.\n\nComputes the value of the LR (CPU) loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.loss_LR_factory","page":"Private APIs","title":"LearningPi.loss_LR_factory","text":"Structure that should be used to construct a LR (CPU) loss function.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_LR_gpu","page":"Private APIs","title":"LearningPi.loss_LR_gpu","text":"Structure that realize a LR gpu loss. This structure can be used as function. The constructor need no paramameters.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_LR_gpu-Tuple{Any}","page":"Private APIs","title":"LearningPi.loss_LR_gpu","text":"Arguments:\n\nπ: a Lagrangian Multipliers Vector,\nexample: an abstract example.\n\nComputes the value of the LR GPU loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.loss_LR_gpu_factory","page":"Private APIs","title":"LearningPi.loss_LR_gpu_factory","text":"Structure that should be used to construct a LR GPU loss function.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_hinge","page":"Private APIs","title":"LearningPi.loss_hinge","text":"Structure of parameters for loss obtained as the inverse of the sub-problem obj value.\n\nFields:\n\n-α: regularization term. Warning: for the moment this parameter not used!\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_hinge-Tuple{Any}","page":"Private APIs","title":"LearningPi.loss_hinge","text":"Arguments:\n\nπ: lagrangian multipliers vector candidate,\nexample: dataset sample object.\n\nComputes the value of the Hinge loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.loss_mse","page":"Private APIs","title":"LearningPi.loss_mse","text":"Structure that realize a MSE loss. This structure can be used as function. The constructor need no paramameters.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_mse-Tuple{Any}","page":"Private APIs","title":"LearningPi.loss_mse","text":"Arguments:\n\nπ: lagrangian multipliers vector candidate,\nexample: dataset sample object,\n\n-_: loss parameters, it should be a structure of type MSELoss.   \n\nReturns the loss function value obtained taking the MSE beteern the predicted Lagrangian multipliers π and the optimal ones in example.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.loss_mse_factory","page":"Private APIs","title":"LearningPi.loss_mse_factory","text":"Structure that should be used to construct a MSE loss function.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_multi_LR","page":"Private APIs","title":"LearningPi.loss_multi_LR","text":"Structure that realize a multi-prediction LR (CPU) loss. This structure can be used as function.\n\nFields:\n\nα: a penalization parameter to weight the different predictions, by default is 0.5,\nlr: a loss of type loss_LR, automatically constructed.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.loss_multi_LR-Tuple{AbstractVector}","page":"Private APIs","title":"LearningPi.loss_multi_LR","text":"Arguments:\n\nπ: a Lagrangian Multipliers Vector,\nexample: an abstract example.\n\nComputes the value of the multi-prediction LR (CPU) loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.loss_multi_LR_factory","page":"Private APIs","title":"LearningPi.loss_multi_LR_factory","text":"Structure that should be used to construct a multi-prediction LR (CPU) loss function.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.lr_features_matrix","page":"Private APIs","title":"LearningPi.lr_features_matrix","text":"with this choice the features matrix considers the informations related to the continuous relaxation of the instance.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.without_cr_features_matrix","page":"Private APIs","title":"LearningPi.without_cr_features_matrix","text":"with this choice the features matrix does not considers the informations related to the continuous relaxation of the instance.\n\n\n\n\n\n","category":"type"},{"location":"api_private/#LearningPi.zero_deviation","page":"Private APIs","title":"LearningPi.zero_deviation","text":"Type to use as deviation vector (i.e. the starting point from which our model produce an additive activation) the all zeros vector. \n\n\n\n\n\n","category":"type"},{"location":"api_private/#ChainRulesCore.rrule-Tuple{LearningPi.loss_LR, AbstractArray}","page":"Private APIs","title":"ChainRulesCore.rrule","text":"Compute the value of the Learning by Experience loss (usining the inverse of value of the sub-problem) and its pullback function.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#ChainRulesCore.rrule-Tuple{LearningPi.loss_LR_gpu, AbstractArray}","page":"Private APIs","title":"ChainRulesCore.rrule","text":"Compute the value of the Learning by Experience loss (usining the inverse of value of the sub-problem) and its pullback function.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#ChainRulesCore.rrule-Tuple{LearningPi.loss_hinge, AbstractArray}","page":"Private APIs","title":"ChainRulesCore.rrule","text":"Compute the value of the Hinge Loss and its pullback function.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#Flux.cpu-Tuple{LearningPi.Graphormer}","page":"Private APIs","title":"Flux.cpu","text":"Arguments:\n\nm: a Graphormer model.\n\nExtends the cpu function of Flux to be applied to Graphormer model.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#Flux.gpu-Tuple{LearningPi.Graphormer}","page":"Private APIs","title":"Flux.gpu","text":"Arguments:\n\nm: a Graphormer model.\n\nExtends the gpu function of Flux to be applied to Graphormer model.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.LM_sign-Tuple{Any, Instances.instanceGA}","page":"Private APIs","title":"LearningPi.LM_sign","text":"Arguments:\n\n-`x`: an unsigned Lagrangian multipliers vector,\n-`ins`: an instances (of type `instanceGA`).\n\nReturn the Lagrangian Multipliers -softplus(x) as for the way in which is encoded GA we have non-positive Lagrangian multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.LM_sign-Tuple{Any, abstractInstanceMCND}","page":"Private APIs","title":"LearningPi.LM_sign","text":"Arguments:\n\n-`x`: an unsigned Lagrangian multipliers vector,\n-`ins`: an instances (of type `abstractInstanceMCND`).\n\nReturn the Lagrangian Multipliers x as for MCND we have no sign constraint..\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.LM_sign-Tuple{Any, instanceCWL}","page":"Private APIs","title":"LearningPi.LM_sign","text":"Arguments:\n\n-`x`: an unsigned Lagrangian multipliers vector,\n-`ins`: an instances (of type `instanceCWL`).\n\nReturn the Lagrangian Multipliers x as for the way in which is encoded CWL we have no-sign constraints for the Lagrangian multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.adj_var_constr-Tuple{Instances.instanceGA}","page":"Private APIs","title":"LearningPi.adj_var_constr","text":"Arguments:\n\n-`ins`: an instances (of type `abstractInstanceGA`),\n\nreturn the adjaciency matrix associated to the dualized constrants and the variables nodes in the bipartite graph representation. The component associated to a couple (constraint,variable) is equal to 1 if and only if the variable has non-null coefficient in constraint. Otherwise is zero.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.adj_var_constr-Tuple{abstractInstanceMCND}","page":"Private APIs","title":"LearningPi.adj_var_constr","text":"Arguments:\n\n-ins: an instances (of type abstractInstanceMCND).\n\nReturn the adjaciency matrix associated to the dualized constrants and the variables nodes in the bipartite graph representation. The component associated to a couple (constraint,variable) is equal to 1 if and only if the variable has non-null coefficient in constraint. Otherwise is zero.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.adj_var_constr-Tuple{instanceCWL}","page":"Private APIs","title":"LearningPi.adj_var_constr","text":"Arguments:\n\n-`ins`: an instances (of type `instanceCWL`).\n\nreturn the adjaciency matrix associated to the dualized constrants and the variables nodes in the bipartite graph representation. The component associated to a couple (constraint,variable) is equal to 1 if and only if the variable has non-null coefficient in constraint. Otherwise is zero.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.aggregate_features-Tuple{abstractInstance, Any, Any}","page":"Private APIs","title":"LearningPi.aggregate_features","text":"Arguments:\n\nins: instance structure,\nvarFeatures: features matrix for the variables of the problem,\nG: adjaciency matrix that have a one in the position for the couple (variable, constraint) if and only if the variable is used in the constraint.\n\nReturns the features associated to the dualized constraint of the instance ins obtained by an aggregation of varFeatures with respect to the neighbourhoods induced by the adjaciency matrix G.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.compareWithBests-Tuple{Dict, Dict, Any, Any}","page":"Private APIs","title":"LearningPi.compareWithBests","text":"Arguments:\n\ncurrentMetrics : a dictionary of Float, \nbestMetrics : a dictionary of Float, \nnn : a neural network,\nendString : a string used to memorize the best models.\n\nThis function compare all the values in bestMetrics with the ones in currentMetrics (that corresponds to the same key). If some value in currentMetrics is better, then we update the correspondent value in bestMetrics and we save the model in a bson file.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.createEmptyDataset-Tuple{LearningPi.learningGNN}","page":"Private APIs","title":"LearningPi.createEmptyDataset","text":"Arguments:\n\nlt: learning Multi Layer Perceptron type.\n\nCreate an empty dataset for the Graph Neural network learning type.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.createEmptyDataset-Tuple{LearningPi.learningMLP}","page":"Private APIs","title":"LearningPi.createEmptyDataset","text":"Arguments:\n\nlt: learning Multi Layer Perceptron type.\n\nCreate an empty dataset for the  Multi Layer Perceptron learning type.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.create_features-Tuple{Instances.instanceGA}","page":"Private APIs","title":"LearningPi.create_features","text":"Arguments:\n\n\t- `ins`: instance object, it should be of type instanceGA.\n\nread the features and returns a features structure.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.create_features-Tuple{cpuInstanceMCND}","page":"Private APIs","title":"LearningPi.create_features","text":"Arguments:\n\nins: instance structure, should be of type cpuInstanceMCND.\n\nCreate and return as output a features structure for the MCND instance ins.  \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.create_features-Tuple{instanceCWL}","page":"Private APIs","title":"LearningPi.create_features","text":"Arguments:\n\n- `ins`: instance object, it should be of type `instanceCWL`. \n\nSolves the Continuous Relaxation and the Lagrangian Sub-Problem considering as Lagrangian Multipliers\nthe dual variables associated to the relaxed constraints and then returns a features structure.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.create_model_gasse","page":"Private APIs","title":"LearningPi.create_model_gasse","text":"Arguments:\n\nwhere_sample: a structure composed by three booleans to say where perform sampling, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nnBlocks: number of main-blocks that compose the core part of the encoder,\nnNodes: dimension of the hidden-space for the features representation,\nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\nact: activation function for the parallel MLP, by default relu,\nact_conv: activation function for the Graph-Convolutional Layers, by default relu,\nseed: random generation seed, by default 1, \nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default [100,250,500],\nhH:   a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, by default [500] ,\nhF:  a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\npDrop: drop-out parameter, by default  0.001 (unused in this implementation, will be removed soon),\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\naggr: the aggregation function, by default mean,\nprediction_layers: a vector that contains the indexes of the layers in which we want perform a prediction, by default [], in this case we use the decoder only in the last main-block of the Graphormer.\n\nReturn a model as defined in Graphormer.jl using the provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_private/#LearningPi.create_model_nair","page":"Private APIs","title":"LearningPi.create_model_nair","text":"Arguments:\n\nwhere_sample: a structure composed by three booleans to say where perform sampling, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nnBlocks: number of main-blocks that compose the core part of the encoder,\nnNodes: dimension of the hidden-space for the features representation,\nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\nact: activation function for the parallel MLP, by default relu,\nact_conv: activation function for the Graph-Convolutional Layers, by default relu (unused in this implementation, will be removed soon),\nseed: random generation seed, by default 1, \nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default [100,250,500],\nhH:   a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, by default [500] ,\nhF:  a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\npDrop: drop-out parameter, by default  0.001 (unused in this implementation, will be removed soon),\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true;\naggr: the aggregation function, by default mean,\nprediction_layers: a vector that contains the indexes of the layers in which we want perform a prediction, by default [], in this case we use the decoder only in the last main-block of the Graphormer.\n\nreturns a model as defined in Graphormer.jl using the provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_private/#LearningPi.create_rms-Tuple{Any}","page":"Private APIs","title":"LearningPi.create_rms","text":"Arguments:\n\nd: size of input in the input layers.\n\nreturn a RMSNorm function.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.deviationFrom-Tuple{Any, LearningPi.cr_deviation}","page":"Private APIs","title":"LearningPi.deviationFrom","text":"Arguments:\n\nx: the bipartite-graph representation of the instance.\n\nFor the cr_deviation it returns the dual variables associated to the dualized constraints in the optimal solution of the continuous relaxation, taking the good components from the nodes features matrix in the bipartite-graph representation.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.deviationFrom-Tuple{Any, LearningPi.zero_deviation}","page":"Private APIs","title":"LearningPi.deviationFrom","text":"Arguments:\n\nx: the bipartite-graph representation of the instance.\n\nFor the zero_deviation it returns an all-zeros vector with the correct size. The size will be the same as the dual variables associated to the dualized constraints in the optimal solution of the continuous relaxation, taking the good components from the nodes features matrix in the bipartite-graph representation.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.features_matrix-Tuple{Instances.instanceGA, Any, LearningPi.abstract_features_matrix}","page":"Private APIs","title":"LearningPi.features_matrix","text":"Arguments:\n\nins: instance structure, it should be a sub-type of instanceGA,\nfeatObj: features object containing all the characteristics,\nfmt: features matrix type.\n\nConstruct the matrix of the features for a bipartite-graph representation of the instance.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.features_matrix-Tuple{abstractInstanceMCND, LearningPi.featuresMCND, LearningPi.abstract_features_matrix}","page":"Private APIs","title":"LearningPi.features_matrix","text":"Arguments:\n\nins: instance structure, it should be a sub-type of abstractInstanceMCND,\nfeatObj: features object containing all the characteristics, \nfmt: features matrix type.\n\nConstruct the matrix of the features for a bipartite-graph representation of the instance.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.features_matrix-Tuple{instanceCWL, Any, LearningPi.abstract_features_matrix}","page":"Private APIs","title":"LearningPi.features_matrix","text":"Arguments:\n\nins: instance structure, it should be a sub-type of instanceCWL,\nfeatObj: features object containing all the characteristics,\nfmt: features matrix type.\n\nConstruct the matrix of the features for a bipartite-graph representation of the instance.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.features_variables-Tuple{Instances.instanceGA, Any, Any}","page":"Private APIs","title":"LearningPi.features_variables","text":"Arguments:\n\n-ins: an instances (of type abstractInstanceGA),\n\nfeatObj: features encoded in an apposite structure,\nG: adjaciency matrix that have a one in the position for the couple (variable, constraint) if and only if the variable is used in the constraint.\n\nReturns the features associated to the variables in ins using the structure of the instance and the featObj.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.features_variables-Tuple{abstractInstanceMCND, Any, Any}","page":"Private APIs","title":"LearningPi.features_variables","text":"Arguments:\n\nins: instance structure (of type abstractInstanceMCND),\nfeatObj: features encoded in an apposite structure,\nG: adjaciency matrix that have a one in the position for the couple (variable, constraint) if and only if the variable is used in the constraint.\n\nReturns the features associated to the variables in ins using the structure of the instance and the featObj.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.features_variables-Tuple{instanceCWL, Any, Any}","page":"Private APIs","title":"LearningPi.features_variables","text":"Arguments:\n\n-ins: an instances (of type instanceCWL),\n\nfeatObj: features encoded in an apposite structure,\nG: adjaciency matrix that have a one in the position for the couple (variable, constraint) if and only if the variable is used in the constraint.\n\nReturns the features associated to the variables in ins using the structure of the instance and the featObj.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.forwardBackward","page":"Private APIs","title":"LearningPi.forwardBackward","text":"Arguments:\n\nloss: a structure that contains the parameters α and β of the loss,\ntrainSet: the training dataset structure,\nnn: the neural network model,\ncurrentMetrics: a dictionary of Float, \nopt: the optimizer used for the training,\nloss: the loss function,\nepoch: the current epoch,\nlt: learning type object,   \ndt: deviation type (0 or duals of the continuous relaxation).\n\nThis function performs the forward-backward pass for the training considering a generic loss and a generic learning type.\n\n\n\n\n\n","category":"function"},{"location":"api_private/#LearningPi.forwardBackward-Tuple{Any, LearningPi.Graphormer, Any, Any, Any, Int64, LearningPi.learningType, LearningPi.abstract_deviation}","page":"Private APIs","title":"LearningPi.forwardBackward","text":"Arguments:\n\ntrainSet: the (training) set,\nnn: a model of type Graphormer,\ncurrentMetrics: a dictionary that contains the metrix of the current iteration,\nopt: an Optimiser,\nloss: loss function,\nepoch: the epcoh counter (this patameter is unsued in the current implementation and it will be soon removed), \nlt: learning type (this patameter is unsued in the current implementation and it will be soon removed),\ndt: deviation type  (this patameter is unsued in the current implementation and it will be soon removed).\n\nThis function performs the forward and backward pass for the model nn over all the (training) set trainSet.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.gap-Tuple{LearningPi.abstract_example, Real, Real, Real}","page":"Private APIs","title":"LearningPi.gap","text":"Arguments:\n\nexample: the current example (dataset point),\nobjPred: the current obective for the example,\nobjGold: the optimal value of the Lagrangian Dual,\nnInst: the number of the instances in the set.\n\nComputes the GAP of the instance in the example using the predicted objective objPred.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.gap_closure-Tuple{LearningPi.abstract_example, Real, Real, Real}","page":"Private APIs","title":"LearningPi.gap_closure","text":"Arguments:\n\nexample: the current example (dataset point),\nobjPred: the current obective for the example,\nobjGold: the optimal value of the Lagrangian Dual,\nnInst: the number of the instances in the set.\n\nComputes the closure GAP of the instance in the example using the predicted objective objPred. The closure is w.r.t. the value of the Lagrangian Sub-Problem, solved with the dual variables of the continuous relaxation.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_cr_features-Tuple{LearningPi.cr_features_matrix, Any, Any, Any}","page":"Private APIs","title":"LearningPi.get_cr_features","text":"Arguments:\n\n-fmt: feature matrix type (it shoul be cr_features_matrix) \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_cr_features-Tuple{LearningPi.lr_features_matrix, Any, Any, Any}","page":"Private APIs","title":"LearningPi.get_cr_features","text":"Arguments:\n\n-fmt: feature matrix type (it should be lr_features_matrix).\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_cr_features-Tuple{LearningPi.without_cr_features_matrix, Any, Any, Any}","page":"Private APIs","title":"LearningPi.get_cr_features","text":"Arguments:\n\n-fmt: feature matrix type (it should be without_cr_features_matrix).\n\nin this case we have no CR features and it returns an empty vector.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_device-Tuple{LearningPi.abstract_loss}","page":"Private APIs","title":"LearningPi.get_device","text":"Arguments:\n\n- `_`: the loss parameters.\n\nreturns the device (cpu/gpu) used to compute the loss.\nFor a general loss will be CPU.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_device-Tuple{LearningPi.loss_LR_gpu}","page":"Private APIs","title":"LearningPi.get_device","text":"Arguments:\n\n_: loss function.\n\nreturns the device to use with this loss. implementationn this case GPU.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_model-Tuple{Flux.Chain}","page":"Private APIs","title":"LearningPi.get_model","text":"Arguments:\n\n-`nn`: neural network model.\n\nIn this case only returns the model nn.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_model-Tuple{LearningPi.Graphormer}","page":"Private APIs","title":"LearningPi.get_model","text":"Arguments:\n\n-`nn`: neural network model of type `Graphormer`.\n\nReturns a cpu version of the model that can be saved using a bson file.    \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_parameters","page":"Private APIs","title":"LearningPi.get_parameters","text":"Arguments:\n\nnn : neural network,\nlt : learning type,\nf :  useless parameter, removed soon.\n\nReturns the model parameters of nn in the case in which nn belsong to lt learning type.\n\n\n\n\n\n","category":"function"},{"location":"api_private/#LearningPi.get_parameters-Tuple{LearningPi.learningType}","page":"Private APIs","title":"LearningPi.get_parameters","text":"Arguments:\n\nnn: a model, sub-type of learningType.\n\nimplementation for the function that. The general rule is that nn is a model and we can directly call the function Flux.params This abstract implementation will be soon removed.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_λ-Tuple{Any}","page":"Private APIs","title":"LearningPi.get_λ","text":"Arguments:\n\nx: the bipartite-graph representation of the instance.\n\nReturns the dual variables associated to the dualized constraints in the optimal solution of the continuous relaxation, taking the good components from the nodes features matrix in the bipartite-graph representation.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.get_λ-Tuple{CUDA.CuArray}","page":"Private APIs","title":"LearningPi.get_λ","text":"Arguments:\n\nx: a GPU features vector.\n\nReturn the dual variables of the CR associated to the dualized constraints. \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.gradient_lsp-Tuple{AbstractVecOrMat, Instances.cpuInstanceGA}","page":"Private APIs","title":"LearningPi.gradient_lsp","text":"Arguments:\n\n- `x`: the solution of the Lagrangian Sub-problem,\n- `ins`: a cpuInstanceCWL structure.\n\nThis function compute and returns the gradient of the sub-problem objective function w.r.t. the Lagrangian Multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.gradient_lsp-Tuple{AbstractVecOrMat, Instances.gpuMCNDinstance}","page":"Private APIs","title":"LearningPi.gradient_lsp","text":"Arguments:\n\n- `x`: the solution of the Lagrangian Sub-problem,\n- `ins`: a cpuInstanceCWL structure.\n\nThis function compute and returns the gradient of the sub-problem objective function w.r.t. the Lagrangian Multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.gradient_lsp-Tuple{AbstractVecOrMat, cpuInstanceCWL}","page":"Private APIs","title":"LearningPi.gradient_lsp","text":"Arguments:\n\n- `x`: the solution of the Lagrangian Sub-problem,\n- `ins`: a cpuInstanceCWL structure.\n\nThis function compute and returns the gradient of the sub-problem objective function w.r.t. the Lagrangian Multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.gradient_lsp-Tuple{AbstractVecOrMat, cpuInstanceMCND}","page":"Private APIs","title":"LearningPi.gradient_lsp","text":"Arguments:\n\n- `x`: the solution of the Lagrangian Sub-problem,\n- `ins`: a cpuInstanceMCND structure.\n\nThis function compute and returns the gradient of the sub-problem objective function w.r.t. the Lagrangian Multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.load_model-Tuple{Any, LearningPi.learningMLP}","page":"Private APIs","title":"LearningPi.load_model","text":"Arguments:\n\n-`nn`: neural network model,\n-`lt`: learning type (of type `learningMLP`).\n\nIn this case only returns the model nn.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.load_model-Tuple{LearningPi.Graphormer, LearningPi.learningGNN}","page":"Private APIs","title":"LearningPi.load_model","text":"Arguments:\n\n-`nn`: neural network model of type `Graphormer`,\n-`lt`: learning type (of type `learningGNN`).\n\nIn this case only returns the model nn.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.preprocess_weight-Tuple{LearningPi.abstract_features_matrix, Real}","page":"Private APIs","title":"LearningPi.preprocess_weight","text":"Arguments:\n\n-fmt: feature matrix type (it should be without_cr_features_matrix).\n\nPreprocess the edge weights of the bipartite graph representation of the instance. One edges correspond to a pair (variable, constraint). In this project are for the moment implemented three choices: \t- all ones weights, \t- weights equal to the coefficients of the variable in the constraints, \t- a modification of the last to assure positive weights.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.printBests-Tuple{Dict, String}","page":"Private APIs","title":"LearningPi.printBests","text":"Arguments:\n\nbestMetrics: a dictionary of float that contains the best values\n\n\t\t   find in the training for altypel the considered metrics,\n\npath: location where print the results in a file.\t\t\t\t\n\nTakes as input the dictionary of the best metrics and print the values in standard output and in a file defined by the path.                \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.printMetrics-Tuple{Dict}","page":"Private APIs","title":"LearningPi.printMetrics","text":"Arguments:\n\ncurrentMetrics: a dictionary of Float.\n\nThis function takes as input the dictionary of the metrics and print the values associated to training and validation sets.    \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.print_best_models-Tuple{String, Dict}","page":"Private APIs","title":"LearningPi.print_best_models","text":"Arguments:\n\nendString: Path where save the models,\nbestModels: Dictionary of the best models (w.r.t different metrics) found so far.\n\nPrint in a file BSON, located in the folder endString the best model found so far.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.print_json-Tuple{Any, Any, Any, Any, cpuMCNDinstanceFactory}","page":"Private APIs","title":"LearningPi.print_json","text":"Arguments:\n\nins: instance structure, it should be of type cpuMCNDinstance,\nlab: labels structure, it should be of type labelsMCND,\nfeat: features structure, it should be of type featuresMCND,\nfileName: the path to the file json where print the data,\nfactory: instance factory should be of type cpuMCNDinstanceFactory.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.print_json-Tuple{Instances.instanceGA, LearningPi.labelsGA, LearningPi.featuresGA, String}","page":"Private APIs","title":"LearningPi.print_json","text":"Arguments:\n\nins: instance structure, it should be of type <: instanceGA,\nlab: labels structure, it should be of type labelsGA,\nfeat: features structure, it should be of type featuresGA,\nfileName: the path to the file json where print the data.\n\nPrint in a JSON format the information contained in ins,feat and lab, in a file in fileName.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.print_json-Tuple{instanceCWL, LearningPi.labelsCWL, LearningPi.featuresCWL, String}","page":"Private APIs","title":"LearningPi.print_json","text":"Arguments:\n\n-ins: instance structure, it should be of type  of instanceCWL,\n\nlab: labels structure, it should be of type labelsCWL,\nfeat: features structure, it should be of type featuresCWL,\nfileName: the path to the file json where print the data.\n\nPrint the information provided in the instance ins, the labels lab and the features feat in a JSON file located in the path fileName.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.read_labels-Tuple{String, Instances.instanceGA}","page":"Private APIs","title":"LearningPi.read_labels","text":"Arguments:\n\n\t- `fileLabel`: the path to the file where to find labels informations,\n\t- `ins`: instance object, it should be of type `instanceGA`. \n\nReads the labels and returns a labels structure.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.read_labels-Tuple{String, abstractInstanceMCND}","page":"Private APIs","title":"LearningPi.read_labels","text":"# Arguments:\n\t- `fileLabel`: the path to the file where to find labels informations,\n\t- `ins`: instance object, it should be of type abstractInstanceMCND. \n\nRead the labels and returns a labels structure.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.read_labels-Tuple{String, instanceCWL}","page":"Private APIs","title":"LearningPi.read_labels","text":"Arguments:\n\n\t- `fileLabel`: the path to the file where to find labels informations\n\t- `ins`: instance object, it should be of type sub-type of `instanceCWL` \n\nReads the labels and returns a labels structure.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.rhs-Tuple{Instances.instanceGA, Any, Any}","page":"Private APIs","title":"LearningPi.rhs","text":"Arguments:\n\n-`ins`: an instances (of type `abstractInstanceGA`),\n- `k`: useless parameter, only for signature,\n- `i`: bin index.\n\nReturn the right hand side of the dualized constraint associated to the bin i in ins.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.rhs-Tuple{abstractInstanceMCND, Any, Any}","page":"Private APIs","title":"LearningPi.rhs","text":"Arguments:\n\n-`ins`: an instances (of type `abstractInstanceMCND`),\n- `k`: commodity index,\n- `i`: vertex index.\n\nReturn the right hand side of the dualized constraint associated to the commodity k and the vertex i in ins.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.rhs-Tuple{instanceCWL, Any, Any}","page":"Private APIs","title":"LearningPi.rhs","text":"Arguments:\n\n-`ins`: an instances (of type `instanceCWL`),\n- `k`: useless parameter, only for signature,\n- `i`: warehouse index.\n\nReturn the right hand side of the dualized constraint associated to the warehouse i in ins.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.saveHP-Tuple{String, Float32, Float32, Vector{Int64}, Any, LearningPi.learningGNN, LearningPi.abstract_features_matrix, LearningPi.abstract_deviation, Any, Int64, Int64, Vararg{Any, 7}}","page":"Private APIs","title":"LearningPi.saveHP","text":"Arguments:\n\nendString: a string used as name for the output file,\nlr: learning rate of the algorithm,\ndecay: decay for the learning rate,\nh: a list of #(hidden layers), each component of h contains the number of nodes in\n\n the associated hidden layer,\n\nopt: optimizer,\nlt: learning type object,     \nfmt: features matrix type,\ndt: deviation type,\nloss: loss function,\nseedDS: random seed for the dataset generation,\nseedNN: random seed for the neural network parameters,\nstepSize: the step size for the decay scheduler of the optimizer,\nnodes_number: size (number of nodes) in the hidden reprensentation between each layer,\nblock_number: number of blocks in the model,\nhI: sizes of Dense layers in the first part, where the nodes features are sent in the hidden space,\nhF: sizes of Dense layers in the final,\ndataPath: path to the instances used in the dataset,\nfactory: instance factory type.\n\nThis function memorize all this hyper parameters in a JSON file.     \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.saveHP-Tuple{String, Float32, Float32, Vector{Int64}, Any, LearningPi.learningType, Any, Int64, Int64, Any}","page":"Private APIs","title":"LearningPi.saveHP","text":"Arguments:\n\nendString: a string used as name for the output file,\nlr: learning rate of the algorithm,\ndecay: decay for the learning rate,\nh: a list of #(hidden layers), each component of h contains the number of nodes in\n\n the associated hidden layer,\n\nopt: optimizer,\nlt: learning type object,     \nloss: loss function,\nseedDS: random seed for the dataset generation,\nseedNN: random seed for the neural network parameters,\nstepSize: the step size for the decay scheduler of the optimizer.\n\nThis function memorize all this hyper parameters in a JSON file.     \n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.sizeFeatures-Tuple{LearningPi.learningGNN, Any}","page":"Private APIs","title":"LearningPi.sizeFeatures","text":"Arguments:\n\nlt: learning type, it should be a sub-type of learningGNN,\ndS: a dataset.\n\nReturn the size of the features matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.sizeFeatures-Tuple{LearningPi.learningMLP, Any}","page":"Private APIs","title":"LearningPi.sizeFeatures","text":"Arguments:\n\nlt : learning type (general),\ndS : dataset (corpus structure).\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.size_features_constraint-Tuple{LearningPi.cr_features_matrix}","page":"Private APIs","title":"LearningPi.size_features_constraint","text":"Arguments:\n\n-fmt: feature matrix type (it should be cr_features_matrix).\n\nreturns the size of the features associated to the constraints. In this case 6.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.size_features_constraint-Tuple{LearningPi.lr_features_matrix}","page":"Private APIs","title":"LearningPi.size_features_constraint","text":"Arguments:\n\n-fmt: feature matrix type (it should be cr_features_matrix).\n\nreturns the size of the features associated to the constraints. In this case 6.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.size_features_constraint-Tuple{LearningPi.without_cr_features_matrix}","page":"Private APIs","title":"LearningPi.size_features_constraint","text":"Arguments:\n\n-fmt: feature matrix type (it should be cr_features_matrix).\n\nreturns the size of the features associated to the constraints. In this case 4.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.size_features_variable-Tuple{LearningPi.cr_features_matrix}","page":"Private APIs","title":"LearningPi.size_features_variable","text":"Arguments:\n\n-fmt: feature matrix type (it should be cr_features_matrix).\n\nreturns the size of the features associated to the variables. In this case 4.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.size_features_variable-Tuple{LearningPi.lr_features_matrix}","page":"Private APIs","title":"LearningPi.size_features_variable","text":"Arguments:\n\n-fmt: feature matrix type (it should be cr_features_matrix).\n\nreturns the size of the features associated to the variables. In this case 4.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.size_features_variable-Tuple{LearningPi.without_cr_features_matrix}","page":"Private APIs","title":"LearningPi.size_features_variable","text":"Arguments:\n\n-fmt: feature matrix type (it should be cr_features_matrix).\n\nreturns the size of the features associated to the variables. In this case 2.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.sub_problem_value-Tuple{Any, Any, Any, LearningPi.abstract_loss}","page":"Private APIs","title":"LearningPi.sub_problem_value","text":"Arguments:\n\n_: lagrangian multipliers vector candidate, \nv: the value of the loss function,\nexample: dataset sample object,\n_: loss parameters.\n\nCompute the value of the sub-problem for the loss for which it cannot be obtained in a smarter way.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.sub_problem_value-Tuple{Any, Any, Any, LearningPi.loss_GAP}","page":"Private APIs","title":"LearningPi.sub_problem_value","text":"Arguments:\n\n_: lagrangian multipliers (are not used in this implementation),\nv: loss function value,\nexample: an abstract_example,\n_: loss function. \n\nCompute the sub-problem value without solving the Lagrangian Sub-Problem, if it is already solved during the computation of the loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.sub_problem_value-Tuple{Any, Any, Any, LearningPi.loss_LR_gpu}","page":"Private APIs","title":"LearningPi.sub_problem_value","text":"Arguments:\n\n_: lagrangian multipliers (are not used in this implementation),\nv: loss function value,\nexample: an abstract_example,\n_: loss function .\n\nCompute the sub-problem value without solving the Lagrangian Sub-Problem, if it is already solved during the computation of the loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.sub_problem_value-Tuple{Any, Any, Any, LearningPi.loss_LR}","page":"Private APIs","title":"LearningPi.sub_problem_value","text":"Arguments:\n\n_: lagrangian multipliers (are not used in this implementation),\nv: loss function value,\n_: an abstract_example,\n_: loss function. \n\nCompute the sub-problem value without solving the Lagrangian Sub-Problem, if it is already solved during the computation of the loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.sub_problem_value-Tuple{Any, Any, Any, LearningPi.loss_hinge}","page":"Private APIs","title":"LearningPi.sub_problem_value","text":"Arguments:\n\n_: lagrangian multipliers vector candidate, \nv: the value of the loss function,\nexample: dataset sample object,\n_: loss parameters, it should be a structure of type HingeLoss.   \n\nCompute the value of the sub-problem without recomputing it, but using the value of the loss function (for the HingeLoss)  and other informations contained in the sample.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.sub_problem_value-Tuple{Any, Any, LearningPi.abstract_example, LearningPi.loss_GAP_closure}","page":"Private APIs","title":"LearningPi.sub_problem_value","text":"Arguments:\n\n_: lagrangian multipliers (are not used in this implementation),\nv: loss function value,\nexample: an abstract_example,\n_: loss function. \n\nCompute the sub-problem value without solving the Lagrangian Sub-Problem, if it is already solved during the computation of the loss.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.testAndPrint-Tuple{Dict, Any, Any, Any, LearningPi.learningType, LearningPi.abstract_deviation}","page":"Private APIs","title":"LearningPi.testAndPrint","text":"function testAndPrint(currentMetrics::Dict,testSet,nn,loss,loss,lt::learningType)\n\nArguments:\n\ncurrentMetrics: a dictionary of Floats that contains several metric for the current epoch, \ntestSet : a vector of example that correspond to the test set,\nnn : a neural network model.\nloss: a structure that encode the loss function.\nlt: learning type object.     \n\nThis function compute different metrics over the validation set. The values are memorized in the dictionary and print them in the standard output.\n\n\n\n\n\n","category":"method"},{"location":"api_private/#LearningPi.validation-Tuple{Dict, Any, Any, Any, LearningPi.learningType, LearningPi.abstract_deviation}","page":"Private APIs","title":"LearningPi.validation","text":"Arguments:\n\ncurrentMetrics: a dictionary of Float, \nvalSet : a vector of gnn_dataset that correspond to the validation set,\nnn : a neural network model,\nloss: a structure with the parameters of the loss,\n\n\t   For other details of the parameters of a certain loss see the definition of the particular structure of the loss,\n\nloss: loss function,\nlt: learning type object,       \ndt: deviation type (0 or dual of the CR).        \n\nThis function compute different metrics over the validation set. The values are memorized in the dictionary.\n\n\n\n\n\n","category":"method"},{"location":"api/#API-index-for-learning_pi.jl","page":"API index","title":"API index for learning_pi.jl","text":"","category":"section"},{"location":"api/","page":"API index","title":"API index","text":"","category":"page"},{"location":"api_public/#Public-API-for-LearningPi.jl","page":"Public APIs","title":"Public API for LearningPi.jl","text":"","category":"section"},{"location":"api_public/","page":"Public APIs","title":"Public APIs","text":"Modules = [LearningPi]\nPrivate = false","category":"page"},{"location":"api_public/#LearningPi.createCorpus","page":"Public APIs","title":"LearningPi.createCorpus","text":"Arguments:\n\nfeatType : the type of the features instance,\nfolder : the path to the directory that contains the json files that defines the instances (and the associated features and labels),\nmaxInstance : a vector with three components,  that say how many instances take for the training/validation/test set,\nseed : a random seed used to select which instaces consider in the training/validation/test sets,\nfactory: type of instance,\npTrain : The percentage of training instances in the provided folder,\npVal : The percentage of validation instances in the provided folder.\n\nCreate a Corpus, that is a structure with three DataSet field for training,validation and test dataset. Note: The percentage for the test set will be 1-pTrain-pVal. It is important to select the percentage of training and validation in such a way that pTrain+pVal<1 and both will be non-negative.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.createDataSet","page":"Public APIs","title":"LearningPi.createDataSet","text":"Arguments:\n\nlt: the learning type.\ndirectory: a list of paths to the instances.\nmaxInstance: the maximum number of instances that we wat consider in the provided directory. By default is equal to -1, that means consider all the instances in the directory.\nfactory: type of instance, the possibilities in this moment are cpuMCNDinstanceFactory() (that is Multi-Commodity Network-Design instances) and cpuCWLinstanceFactory() (for the Bin Packing instances).\n\nCreate the dataset for the provided (general) learning type. return a dataSet structure of a proper type.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.createDataSet-2","page":"Public APIs","title":"LearningPi.createDataSet","text":"Arguments:\n\n\t- `lt`:learning type, it should be a sub-type of `learningGNN`, \n\t- `directory`: the path to the directory containing instances,\n\t- `maxInstance`: maximum instance number, \n\t- `factory`: instance factory, generic sub-type of `abstractInstanceFactory`.\n\nCreate and return a dataset for the provided learning type `lt`, considering `maxInst` instances of the factory `factory`, contained in `directory`\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.createKfold","page":"Public APIs","title":"LearningPi.createKfold","text":"Arguments:\n\nfeatType : the type of the features instance,\nfolder : the path to the directory that contains the json files that defines the instances (and the associated features and labels),\nmaxInstance : a vector with three components,  that say how many instances take for the training/validation/test set,\nseed : a random seed used to select which instaces consider in the training/validation/test sets,\nfactory: type of instance,\nk: the fold that we want select as test set. Note: 1 <= k <= 10.\n\nCreate a Corpus, that is a struct with three DataSet field for training/validation/test set.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.createLabels-Tuple{Any, Any, Any, Any, Any, abstractInstanceMCND}","page":"Public APIs","title":"LearningPi.createLabels","text":"Arguments:\n\nπ: a (optimal) Lagrangian multipliers vector\nx: the flow variables in the Lagrangian sub-problem, obtained afer the resolution of the sub-problem with multipliers π,\ny: the design variables in the Lagrangian sub-problem, obtained afer the resolution of the sub-problem with multipliers π,\nLRarcs: a vector containign the bounds for each edge of the Lagrangian sub-problem considering π as Lagrangian multipleirs vector,\nobjLR: the bound of the Lagrangian sub-problem considering π as Lagrangian multipleirs vector,\nins: the instance structure (standard instance formulation, without regularization).\n\nReturn a proper label structure.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.createLabels-Tuple{Any, Any, Any, Any, instanceCWL}","page":"Public APIs","title":"LearningPi.createLabels","text":"Arguments:\n\n-π: optimal lagrangian multipliers vector, -x: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the optimal Lagrangian multipliers), -y: primal solution of the Knapsack Lagrangian Relaxation associated to the variables say if we use or not a pack (using the optimal Lagrangian multipliers), -objLR: optimal value of the Lagrangian Dual,\n\nins: instance object, it should be of type sub-type of instanceCWL. \n\nGiven all the fields construct a label structure for the Capacitated Warehouse Location Problem.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.createLabels-Tuple{Any, Any, Any, Instances.instanceGA}","page":"Public APIs","title":"LearningPi.createLabels","text":"Arguments:\n\n-`π`: optimal lagrangian multipliers vector,\n-`x`: primal solution of the Knapsack Lagrangian Relaxation associated to the variables that associate one items to a pack (using the optimal Lagrangian multipliers),\n-`objLR`: optimal value of the Lagrangian Dual,\n- `ins`: instance object, it should be of type `instanceGA`.\n\nGiven all the fields construct a label structure for the Generalized Assignment Problem.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.create_example-Tuple{LearningPi.learningGNN, String, abstractInstanceFactory, LearningPi.abstract_features_matrix}","page":"Public APIs","title":"LearningPi.create_example","text":"Arguments:\n\nlt: learning Type, this function works for all the learning types that use a graph representation of the instance,\nfileName: path to the json that contains all the information to construct the learning sample starting from an instance, its features and the labels,\nfactory: instance factory, it works for all the factory.\n\nReturns an gnnExample_instance with all the information useful for the training.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.create_example-Tuple{LearningPi.learningMLP, String, abstractInstanceFactory, Any}","page":"Public APIs","title":"LearningPi.create_example","text":"Arguments:\n\nlt: learning type, it should be learningMLP,\nfileName: the name of the file json that contains the informations about the instance, its features and its labels,\nfactory: type of instance (it handle both with the normalized and un-normalized instances).\n\nCreate a structure containing the instance, the extracted features and the associated labels.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.create_loss","page":"Public APIs","title":"LearningPi.create_loss","text":"Arguments:\n\n-:: loss parameters, it should be a structure of type HingeLoss.   \n\nreturn the loss correspondent to loss paramameters of type HingeLoss.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_loss-Tuple{LearningPi.loss_GAP_closure_factory}","page":"Public APIs","title":"LearningPi.create_loss","text":"Arguments:\n\n_: a factory, for this implentation it should be of type ~loss_GAP_closure_factory.\n\nReturn a loss corresponding to the factory.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.create_loss-Tuple{LearningPi.loss_GAP_factory}","page":"Public APIs","title":"LearningPi.create_loss","text":"Arguments:\n\n_: a factory, for this implentation it should be of type ~loss_GAP_factory.\n\nReturn a loss corresponding to the factory.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.create_loss-Tuple{LearningPi.loss_LR_factory}","page":"Public APIs","title":"LearningPi.create_loss","text":"Arguments:\n\n_: a factory, for this implentation it should be of type ~loss_LR_factory.\n\nReturn a loss corresponding to the factory.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.create_loss-Tuple{LearningPi.loss_LR_gpu_factory}","page":"Public APIs","title":"LearningPi.create_loss","text":"Arguments:\n\n_: a factory, for this implentation it should be of type ~loss_LR_gpu_factory.\n\nReturn a loss corresponding to the factory.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.create_loss-Tuple{LearningPi.loss_mse_factory}","page":"Public APIs","title":"LearningPi.create_loss","text":"Arguments:\n\n_: a factory, for this implentation it should be of type ~loss_mse_factory.\n\nReturn a loss corresponding to the factory.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.create_model","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nlType: learning type, should be learningTransformer, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nh:  a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, \nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\na: activation function, by default relu,\nseed: random generation seed, by default 1,\nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default  [500, 250, 100],\nhF: a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\nblock_number: number of main-blocks that compose the core part of the encoder, by default 5,\nnodes_number: dimension of the hidden-space for the features representation, by default 500,\npDrop: drop-out parameter, by default 0.001,\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\nfinal_A: final activation function (in the space of Lagrangian multipliers, but before deviation), by default identity.\n\nReturns the neural network model for learningTransformer and the other provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_model-2","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nlType: learning type, should be learningSampleGasse, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nh:  a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, \nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\na: activation function, by default relu,\nseed: random generation seed, by default 1,\nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default  [500, 250, 100],\nhF: a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\nblock_number: number of main-blocks that compose the core part of the encoder, by default 5,\nnodes_number: dimension of the hidden-space for the features representation, by default 500,\npDrop: drop-out parameter, by default 0.001,\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\nfinal_A: final activation function (in the space of Lagrangian multipliers, but before deviation), by default identity.\n\nReturns the neural network model for learningSampleGasse and the other provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_model-3","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nwhere_sample: a structure composed by three booleans to say where perform sampling, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nnBlocks: number of main-blocks that compose the core part of the encoder,\nnNodes: dimension of the hidden-space for the features representation,\nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\nact: activation function for the parallel MLP, by default relu,\nact_conv: activation function for the Graph-Convolutional Layers, by default relu,\nseed: random generation seed, by default 1, \nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default [100,250,500],\nhH:   a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, by default [500] ,\nhF:  a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\npDrop: drop-out parameter, by default  0.001,\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\naggr: the aggregation function, by default mean,\nprediction_layers: a vector that contains the indexes of the layers in which we want perform a prediction, by default [], in this case we use the decoder only in the last main-block of the Graphormer.\n\nreturns a model as defined in Graphormer.jl using the provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_model-4","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nlType: learning type, should be learningMultiPredSample, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nh:  a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, \nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\na: activation function, by default relu,\nseed: random generation seed, by default 1,\nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default  [500, 250, 100],\nhF: a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\nblock_number: number of main-blocks that compose the core part of the encoder, by default 5,\nnodes_number: dimension of the hidden-space for the features representation, by default 500,\npDrop: drop-out parameter, by default 0.001,\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\nfinal_A: final activation function (in the space of Lagrangian multipliers, but before deviation), by default identity.\n\nReturns the neural network model for learningMultiPredSample and the other provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_model-5","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nlType: learning type, should be learningMultiPredTransformer, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nh:  a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, \nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\na: activation function, by default relu,\nseed: random generation seed, by default 1,\nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default  [500, 250, 100],\nhF: a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\nblock_number: number of main-blocks that compose the core part of the encoder, by default 5,\nnodes_number: dimension of the hidden-space for the features representation, by default 500,\npDrop: drop-out parameter, by default 0.001,\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\nfinal_A: final activation function (in the space of Lagrangian multipliers, but before deviation), by default identity.\n\nReturns the neural network model for learningMultiPredTransformer and the other provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_model-6","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nlType: learning type, should be learningSampleTransformer, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nh:  a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, \nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\na: activation function, by default relu,\nseed: random generation seed, by default 1,\nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default  [500, 250, 100],\nhF: a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\nblock_number: number of main-blocks that compose the core part of the encoder, by default 5,\nnodes_number: dimension of the hidden-space for the features representation, by default 500,\npDrop: drop-out parameter, by default 0.001,\ndt : deviation type, by default cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\nfinal_A: final activation function (in the space of Lagrangian multipliers, but before deviation), by default identity.\n\nReturns the neural network model for learningSampleTransformer and the other provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_model-7","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nlType: learning type, should be learningSampleOutside, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nh:  a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, \nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\na: activation function, by default relu,\nseed: random generation seed, by default 1,\nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default  [500, 250, 100],\nhF: a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\nblock_number: number of main-blocks that compose the core part of the encoder, by default 5,\nnodes_number: dimension of the hidden-space for the features representation, by default 500,\npDrop: drop-out parameter, by default 0.001,\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\nfinal_A: final activation function (in the space of Lagrangian multipliers, but before deviation), by default identity.\n\nReturns the neural network model for learningSampleOutside and the other provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_model-8","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nlType: learning type, should be learningSampleNair, \nin: size of the input of the neural network, for each node in the bipartite graph-representation, \nh:  a vector containing the number of nodes in the hidden layers that composes the  MLP inside the main-blocks of the Encoder, \nout: the dimention of the output of the neural network model, for each dualized constraint, by default 1,\na: activation function, by default relu,\nseed: random generation seed, by default 1,\nhI: a vector containing the number of nodes in the hidden layers that composes the initial MLP that send the features into the hidden space representation, by default  [500, 250, 100],\nhF: a vector containing the number of nodes in the hidden layers that composes the final MLP in the Decoder, by default [500, 250, 100],\nblock_number: number of main-blocks that compose the core part of the encoder, by default 5,\nnodes_number: dimension of the hidden-space for the features representation, by default 500,\npDrop: drop-out parameter, by default 0.001,\ndt : deviation type, by default  cr_deviation(),\nstd: standard deviation used for the initialization of the nn parameters, by default 0.00001,\nnorm: a boolean to say if normalize or not during the GNN message passing, by default true,\nfinal_A: final activation function (in the space of Lagrangian multipliers, but before deviation), by default identity.\n\nReturns the neural network model for learningSampleNair and the other provided hyper-parameters.\n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.create_model-9","page":"Public APIs","title":"LearningPi.create_model","text":"Arguments:\n\nlType: general learningType,\nin: size of the input layer,\nh: a vector with the same length as the desired number of hidden layers and each component say how many nodes we want in the correspondent hidden layer,\nout: size of the output layer, by default is equal to one,\na: the activation function for the hidden layers, by default is relu.\n\nThis function creates a model for the provided learning type (in order to use this variant it should be learningArc). The model is a multi layer perceptron with in nodes in the first layer, length(h) hidden layers (the i-th layer has h[i] nodes) and out nodes in the output layer (by default 1). By default each hidden layer use a relu activation function (the input and output layers have no activation function).   \n\n\n\n\n\n","category":"function"},{"location":"api_public/#LearningPi.dataLoader-Tuple{String, Instances.cpuGAinstanceFactory}","page":"Public APIs","title":"LearningPi.dataLoader","text":"Arguments\n\n- `fileName` : a path to a json file that contains the data for the instance, features and labels\n- `factory` : an instance factory for the Generalized Assignment problem\n\nThis function reads the instance, the features and the labels from the json and returns three structures that contains all the informations.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.dataLoader-Tuple{String, MCNDinstanceFactory}","page":"Public APIs","title":"LearningPi.dataLoader","text":"Arguments\n\n- `fileName` : a path to a json file that contains the data for the instance, features and labels,\n- `factory` : an instance object (for the same instance as the features file).\n\nIt reads the instance, the features and the labels from the json and returns three structures that contains all the informations.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.dataLoader-Tuple{String, cpuCWLinstanceFactory}","page":"Public APIs","title":"LearningPi.dataLoader","text":"Arguments\n\n- `fileName` : a path to a json file that contains the data for the instance, features and labels,\n- `factory` : an instance factory for the Capacitated Warehouse Location problem.\n\nIt reads the instance, the features and the labels from the json located in fileName and returns three structures that contains all the informations.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.featuresExtraction-Tuple{LearningPi.learningMLP, Any, abstractInstance}","page":"Public APIs","title":"LearningPi.featuresExtraction","text":"Arguments:\n\nfeatType features type,\nfeatures a features matrix,\nnbFeatures the number of features.\n\nVectorization function for the features when we consider a learningNodeDemand encoding.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.featuresExtraction-Tuple{LearningPi.learningType, Any, Instances.instanceGA, LearningPi.abstract_features_matrix}","page":"Public APIs","title":"LearningPi.featuresExtraction","text":"Arguments:\n\nlt: learnign type, it should be a sub-type of learningType,\nfeatObj: features object containing all the characteristics, \nins: instance structure, it should instanceGA,\nfmt: features matrix type.\n\nReturns the bipartite graph representation with the associated nodes-features matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.featuresExtraction-Tuple{LearningPi.learningType, Any, abstractInstanceMCND, LearningPi.abstract_features_matrix}","page":"Public APIs","title":"LearningPi.featuresExtraction","text":"Arguments:\n\nlt: learnign type, it should be a sub-type of learningGNN,\nfeatObj: features object containing all the characteristics,\nins: instance structure, it should be a sub-type of abstractInstanceMCND.\n\nReturns the bipartite graph representation with the associated nodes-features matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.featuresExtraction-Tuple{LearningPi.learningType, Any, instanceCWL, LearningPi.abstract_features_matrix}","page":"Public APIs","title":"LearningPi.featuresExtraction","text":"Arguments:\n\nlt: learnign type, it should be a sub-type of learningType,\nfeatObj: features object containing all the characteristics, \nins: instance structure, it should instanceCWL,\nfmt: features matrix type.\n\nReturns the bipartite graph representation with the associated nodes-features matrix.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.prediction-Tuple{Flux.Chain, Any, Any, LearningPi.learningMLP, LearningPi.abstract_deviation}","page":"Public APIs","title":"LearningPi.prediction","text":"Arguments:\n\n-nn: neural network model, -f: features matrix, -ins: structure containing the instance informations, -lt: learning type (general).\n\nprovide the predicted Lagrangian multipliers.\n\n\n\n\n\n","category":"method"},{"location":"api_public/#LearningPi.train-Tuple{Int64, LearningPi.Corpus, Any, Flux.Optimise.Optimiser, LearningPi.abstract_loss}","page":"Public APIs","title":"LearningPi.train","text":"Arguments:\n\nmaxEp: the maximum number of epochs for the learning algorithm,\ndS: the Corpus structure that contains the training, validation and test sets,\nnn: the neural network model,\nopt: the optimizer used for the training,\nloss: a structure that contains the parameters α and β of the loss,\nprintEpoch: the number of epochs in which print the metrics of the training,\nendString: the string used to memorize the output files as best models and tensorboard logs,\ndt: deviation type, it could deviate from zero or the duals of the continuous relaxation,\nlt: learning type,\nseed: random seed for the random generators,\nbs batch size.\n\nThis function performs the learning with the provided inputs and save the best models in a bson file.\n\n\n\n\n\n","category":"method"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"This page is only devoted to describe how easily launch the training and the test.","category":"page"},{"location":"Training/#Training","page":"Training Scripts","title":"Training","text":"","category":"section"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"The training the GNN models presented in the paper can be performed using the script in runKfoldTransformer.jl","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"For example, assuming that we are in the main directory of the project, with the command:","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"julia --project=. ../../src/runKfoldTransformer.jl --lr 0.0001 --seed 1 --decay 0.9 --opt rADAM --MLP_i 250 --MLP_h 1000 --MLP_f 250 --lossType LRloss --lossParam 0.9 --maxEp 300 --stepSize 1000000 --kFold 1 --data /users/demelas/MCNDsmallCom40/ --block_number 5 --nodes_number 500 --pDrop 0.25 --factory cpuMCNDinstanceFactory --learningType learningSampleTransformer --cr_deviation true --cr_features true","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"we train one learningSampleTransformer (learningType) model composed by 5 blocks (block_number) and an hidden state representation of size 500 (--nodes_number). The first MLP that goes from the features space to the hidden representation is composed of only one hidden layer with 250 nodes (--MLP_i). Also the Decoder has only one hidden layer of size 250  (--MLP_f). Instead the hidden MLP always goes from the hidden state representation to a space 2 times bigger (--MLP_h). The drop-out probability is setted to 0.25 (--pDrop).","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"The training is performed on the dataset composed by cpuMCNDinstanceFactory (--factory) that can be found in the path /users/demelas/MCNDsmallCom40/ (--data). To construct the dataset we use the seed 1 (--seed) and it will be the same seed used to construct and initialize the model. When the dataset is divised in folds (using the provided seed) the fold 1 is selected as test set ('–kFold`). Notice that even if we change only the seed, but keep the same k-fold, actually the training, validation and test will not be the same.","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"The Optimizer used for the training is rADAM (--opt) with learning rate 0.0001 (--lr) and decay 0.9 (--decay). The step size for the decay (i.e. number of samples before update the learning rate) is 1000000.","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"The model, tensorboard logs and other information about the training/validation/test sets and the hyper-parameters choices are saved in a sub-directory of the directory run that will be created in the directory where we launch the training.","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"To launch the experiments we use slurm file of the type. ","category":"page"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"#!/usr/bin/env bash\n#SBATCH --job-name=T-1-5-s0\n#SBATCH --nodes=1 \n#SBATCH --ntasks-per-node=1    \n#SBATCH --cpus-per-task=1     \n#SBATCH --ntasks=1 \n#SBATCH --gres=gpu:1       \n#SBATCH --qos=qos_gpu_t4        \n#SBATCH --output=output.txt    \n#SBATCH --error=error.txt \n\nexport JULIA_NUM_THREADS=1\njulia --project=../.. ../../src/runKfoldTransformer.jl --lr 0.0001 --seed 1 --decay 0.9 --opt ADAM --MLP_i 250 --MLP_h 1000 --MLP_f 250 --lossType LRloss --lossParam 0.9 --maxEp 300 --stepSize 1000000 --kFold 1 --data /users/demelas/MCNDsmallCom40/ --block_number 5 --nodes_number 500 --pDrop 0.25 --factory cpuMCNDinstanceFactory --learningType learningSampleTransformer --cr_deviation true --cr_features true","category":"page"},{"location":"Training/#Testing","page":"Training Scripts","title":"Testing","text":"","category":"section"},{"location":"Training/","page":"Training Scripts","title":"Training Scripts","text":"Documentation Work in Progress...","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LearningPi","category":"page"},{"location":"#Learning-Lagrangian-Multipliers","page":"Home","title":"Learning Lagrangian Multipliers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LearningPi.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package provides the implementation of the learning framework presented in: ","category":"page"},{"location":"","page":"Home","title":"Home","text":"F. Demelas, J. Le Roux, M. Lacroix, A. Parmentier \"Predicting Lagrangian Multipliers for Mixed Integer Linear Programs\", ICML 2024. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"It depends on the package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"https://github.com/FDemelas/Instances","category":"page"},{"location":"","page":"Home","title":"Home","text":"where we develop the instance encoding and the resolution of the Lagrangian Sub-Problem and the Continuous Relaxation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The dataset used in the paper can be found at:","category":"page"},{"location":"","page":"Home","title":"Home","text":"https://github.com/FDemelas/datasets_learningPi","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install all the dependencies it is sufficient to use this commands in Julia:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg;\nPkg.activate(\".\");\nPkg.instantiate();","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you find some problem with the package Instances.jl you can install it using:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.activate(\".\");\nPkg.add(url=\"https://github.com/FDemelas/Instances\")\nPkg.instantiate();","category":"page"},{"location":"","page":"Home","title":"Home","text":"then the package can be used ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LearningPi","category":"page"},{"location":"#License","page":"Home","title":"License","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This work is licensed under a [Creative Commons Attribution 4.0 International License][cc-by].","category":"page"},{"location":"","page":"Home","title":"Home","text":"[![CC BY 4.0][cc-by-image]][cc-by]","category":"page"},{"location":"","page":"Home","title":"Home","text":"[cc-by]: http://creativecommons.org/licenses/by/4.0/ [cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png [cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg","category":"page"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"In this section we will provided further details about the Sampling Mechanism implemented in the Package.","category":"page"},{"location":"Sampling/#Sampling-Type","page":"Sampling Mechanism","title":"Sampling Type","text":"","category":"section"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"The package dispose of only one type of Sampling.","category":"page"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"Anyway this sampling can be enbedded into the models in three differents ways, providing different architectures:","category":"page"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"In the hidden space representation as [1], before use the decoder.\nIn all the hidden space representations. As the previous the sampling is performed in the hidden space, the difference is that the sampling in this case is performed between all the blocks and not only before the decoder. \nIn the output space, in this case the decoder returns one vector of size two for each dualized constraint representing the mean and the standard deviation of a Gaussian distribution that directly sample in the Lagrangian Multipliers Space.","category":"page"},{"location":"Sampling/#References","page":"Sampling Mechanism","title":"References","text":"","category":"section"},{"location":"Sampling/","page":"Sampling Mechanism","title":"Sampling Mechanism","text":"[1]: F. Demelas, J. Le Roux, M. Lacroix, A. Parmentier \"Predicting Lagrangian Multipliers for Mixed Integer Linear Programs\", ICML 2024.","category":"page"},{"location":"Models/#General-Models","page":"Machine Learning Models","title":"General Models","text":"","category":"section"},{"location":"Models/#MLP","page":"Machine Learning Models","title":"MLP","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"A Classic Multi Layer Perceptron that receeives as input one features vector (for each dualized constraint) and predict (in parallel) one Lagrangian Multiplier Value (for each relaxed constraint).","category":"page"},{"location":"Models/#GNN-Models","page":"Machine Learning Models","title":"GNN Models","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"The key models of this project belongs from this family and can be all seen as particular instantiation of the more general model implemented in Graphormer.jl.","category":"page"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"The key component of this model consists on the block presented in [1].","category":"page"},{"location":"Models/#Some-Easy-to-Repeat-Models","page":"Machine Learning Models","title":"Some Easy-to-Repeat Models","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"In this section we will present some GNN models that are already implemented as partticular instantiation of the model defined in Graphormer.jl.","category":"page"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"All the constructors for this models can be found in the file ModelFactory.jl.","category":"page"},{"location":"Models/#LearningTransformer","page":"Machine Learning Models","title":"LearningTransformer","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"Simply consists on sequential Chain of the block presented in [1], without Sampling Mechanism.","category":"page"},{"location":"Models/#LearningSampleTransformer","page":"Machine Learning Models","title":"LearningSampleTransformer","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"As LearningTransformer, the only difference is that for this model we consider the Sampling mechanism, as presented in [1]. ","category":"page"},{"location":"Models/#LearningSampleGasse","page":"Machine Learning Models","title":"LearningSampleGasse","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"As LearningSampleTransformer this architecture consider the same Sampling Mechanism presented in [1]. Insted of using ours architecture it use one more near to the one presented by Gasse et al. in [2].","category":"page"},{"location":"Models/#LearningSampleNair","page":"Machine Learning Models","title":"LearningSampleNair","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"As LearningSampleTransformer this architecture consider the same Sampling Mechanism presented in [1]. Insted of using ours architecture it use one more near to the one presented by Nair et al. in [3]","category":"page"},{"location":"Models/#LearningSampleOutside","page":"Machine Learning Models","title":"LearningSampleOutside","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"As LearningTransformer, the only difference is that for this model we consider a sampling mechanism. While LearningSampleTransformer sample in the hidden space (as presented in [1]), in this case we sample directly in the output space. More details on the sampling mechanism can be found in the apposite Section. ","category":"page"},{"location":"Models/#LearningMultiPredTransformer","page":"Machine Learning Models","title":"LearningMultiPredTransformer","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"This model as the same inner structure as LearningTransformer, but it contains several decoders and so is able to provide several Lagrangian Multipliers prediction using the same model (maximum one for block). The model LearningTransformer can be seen as this model with only one decoder at the end of the Block Chain. No sample mechanism is used in this case.","category":"page"},{"location":"Models/#LearningMultiPredSample","page":"Machine Learning Models","title":"LearningMultiPredSample","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"This model as the same inner structure as LearningSampleTransformer, but it contains several decoders and so is able to provide several Lagrangian Multipliers prediction using the same model (maximum one for block). The model LearningSampleTransformer can be seen as this model with only one decoder at the end of the Block Chain. The sampling mechanism is the same as LearningSampleTransformer for each predicted Lagrangian Multipliers vector.","category":"page"},{"location":"Models/#References:","page":"Machine Learning Models","title":"References:","text":"","category":"section"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"[1]: F. Demelas, J. Le Roux, M. Lacroix, A. Parmentier \"Predicting Lagrangian Multipliers for Mixed Integer Linear Programs\", ICML 2024.","category":"page"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"[2]: Gasse, M., Chételat, D., Ferroni, N., Charlin, L., and Lodi, A. Exact Combinatorial Optimization with Graph Convolutional Neural Networks. In Wallach, H., Larochelle, H., Beygelzimer, A., Alché-Buc, F. d., Fox, E., and Garnett,R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.","category":"page"},{"location":"Models/","page":"Machine Learning Models","title":"Machine Learning Models","text":"[3]: Nair, V., Bartunov, S., Gimeno, F., von Glehn, I., Lichocki, P., Lobov, I., O’Donoghue, B., Sonnerat, N., Tjandraatmadja, C., Wang, P., Addanki, R., Hapuarachchi, T., Keck, T., Keeling, J., Kohli, P., Ktena, I., Li, Y., Vinyals, O., and Zwols, Y. Solving mixed integer programs using neural networks. CoRR, abs/2012.13349, 2020.","category":"page"}]
}
